# Component-Based Software Design Team’s Whitepaper for the Numerical Technology Modernization Program


Version: 1.0.0  
Date: 2019.03.18  
Written by: Corey Trahan, Lucas Pettey, and Chris Massey (Chris.Massey@usace.army.mil)


## Introduction 

The Component-Based Software Design Team was created as part of the overall Numerical Technology Modernization Program (NTMS). The NTMS was designed to improve and integrate the Coastal and Hydraulics Lab’s (CHL) relevant numerical technologies, which in turn produces more robust models and field data that are efficient to access, apply and cost less to use and maintain. Part of the NTMS strategy is for a team to be formed in order to make recommendations on how best to migrate existing software to a Component-Based Software Design (CBSD). The CBSD Team was tasked to emphasize modularity of code through the separation of functionality available throughout software systems. It is a reuse-based approach to defining and implementing independent components that can be coupled to meet various needs. The strategy requires modular components that must be substitutable, enabling components to be replaced with either an updated version or an alternative without breaking the system in which the component operates. Programmers will design and implement software components to be reusable and extensible. The ERDC transition to component-based software development is a long-term strategy and one that will require dedicated effort and funding. Several tactical steps have already been initiated that enable code-level compartmentalization and coupling. These initial steps will eventually lead to process level functional components and coupling abilities at the process level for even finer granularity.
Making the transition from our current numerical model practices will require significant software re-engineering of existing codes and transitioning current and future development processes to make use of CBSD. The CBSD plan improves efficiency and reduces time from R&D to solution for new physical theories and computational methods by highly modularizing tools creating flexible computational environments to streamline this process. This task covers design and implementation of multiple layers of Application Programming Interfaces (API’s) to support novel coupling of existing computational models and rapid development for complex, multi-physics applications. The following steps have been identified as critical to successful implementation of this overall plan:  

1. Modularize all required software at the model level beginning with an inventory of all models, software, codes and their requirements with complete documentation of precise formulations of the set of processes covered by each.  
2. Create a detailed process, technical team, and schedule to allow for process-level modularization based on selected existing operational codes.  
3. Complete model level modularization of all supported codes and document code changes that must be incorporated into official code documentation.  
4. Develop standardized process level modules based on those common processes identified during model modularization. Examples could include a meteorology module (winds/pressures/precipitation), friction module, sediment module, and a standardized output file module to list a few examples. Modularized codes and associated code documentation will be setup in a version control repository for access across CHL and by external development partners, adhering to the numerical technology framework described above.  
5. Complete process level modularization of selected models by implementing the process modules developed in 4.

An important element of component-based software is the ability to couple that software with other codes. In order to achieve this in an efficient way, the models need to be modularized into three basic parcels, an initialization phase, a run phase and a finalization phase. Equally import to this kind of modularization, is the need to move away from global data structures that are non-reentrant. Instead data structures need to be abstracted where internal representations of a data type are distinct from its external viewable/callable state. In short, this will allow multiple instances of the same model to be run within the same memory space without conflicts. The AdH suite has already made substantial progress toward this process.

This report serves to complete the documentation required in the “Model Level Modularization Plan and Documentation” and the “Create Plan and Team for Process Level Components” plan mentioned in the numerated list above.

The CHL makes use of many different numerical models and technologies, more than 30 and less than 100 different models/technologies, some of which include approved models but not recommended models by the HH&C COP. In an effort to make the CBSP achievable, the following software/models have been identified as candidates for an initial modernization evaluation and are listed in no particular order.

**Adaptive Hydraulics (AdH)** - AdH is a two and three dimensional unstructured finite element based suite of codes for solving baroclinic and barotropic flow conditions for shallow water (SW). It is routinely used for modeling sediment transport and is linked with water quality and species models for environmental studies. It supports a host of features vital to most hydraulic and transport-engineering applications, including spatial and temporal adaption, surface wave and wind-wave stress coupling, flow through hydraulic structures (locks, weirs, flap gate, etc.) and vessel flow interactions. Typical AdH applications include simulations of unsteady flows (tidal, hydrograph, etc.) and transport in rivers, estuaries and reservoirs. To remain on the forefront of ERDC-EQM development, AdH is also internally linked to process-oriented libraries for cohesive/non-cohesive sediment transport, meteorology, friction and turbulence applications.

**The ADvanced CIRCulation model (ADCIRC)** – ADCIRC is best known as a two-dimensional, depth-integrated, barotropic time-dependent long wave, hydrodynamic circulation model. It also has a three dimensional shallow water formulation for both baroclinic and barotropic flows. ADCIRC models can be applied to computational domains encompassing the deep ocean, continental shelves, coastal seas, and small-scale estuarine systems. Typical ADCIRC applications include modeling tides and wind driven circulation, analysis of hurricane storm surge and flooding, dredging feasibility and material disposal studies, larval transport studies, near shore marine operations. ADCIRC is a highly developed computer program for solving the equations of motion for a moving fluid on a rotating earth. These equations have been formulated using the traditional hydrostatic pressure and Boussinesq approximations and have been discretized in space using the finite element (FE) method and in time using the finite difference (FD) method.

**CMS FLOW** - The hydrodynamic circulation model CMS-Flow is a component of the Coastal Modeling System developed by the US Army Corps of Engineers Coastal and Hydraulics Laboratory. CMS-Flow is a two-dimensional, finite-difference numerical approximation of the depth-integrated continuity and momentum equations. Cells are defined on a staggered, rectilinear grid and can have constant or variable side lengths. The momentum equations are solved in a time-stepping manner first, followed by solution of the continuity equation, in which the updated velocities calculated by the momentum equations are applied. The model simulates currents, water level, sediment transport, and morphology in the coastal zone. CMS-Flow was designed to model navigation channel performance and sediment exchanges between the inlet and adjacent beaches in the coastal zone.

**Gridded Surface Subsurface Hydrologic Analysis (GSSHA)** – GSSHA is a multidimensional modeling technology that uniformly couples overland, surface, and subsurface flow for accurate watershed simulation. It is a physics-based, distributed, hydrologic, sediment and constituent fate and transport model developed by the ERDC. GSSHA is routinely applied for both civil works and military applications.

**WaveWatch III** – WaveWatch III is a third generation unstructured wave model developed at NOAA/NCEP in the spirit of the WAM model (WAMDIG 1988, Komen et al. 1994). It is a further development of the model WAVEWATCH, as developed at Delft University of Technology (Tolman 1989, 1991a) and WAVEWATCH II, developed at NASA, Goddard Space Flight Center (e.g., Tolman 1992). WAVEWATCH III®, however, differs from its predecessors in many important points such as the governing equations, the model structure, the numerical methods and the physical parameterizations. Furthermore, with model version 3.14, WAVEWATCH III® is evolving from a wave model into a wave modeling framework, which allows for easy development of additional physical and numerical approaches to wave modeling.

**STWAVE** - STWAVE is a steady-state, finite difference, spectral model based on the wave action balance equation. STWAVE simulates depth-induced wave refraction and shoaling, current-induced refraction and shoaling, depth- and steepness-induced wave breaking, diffraction, wave growth because of wind input, and wave-wave interaction and white capping that redistribute and dissipate energy in a growing wave field. The purpose of STWAVE is to provide an easy-to-apply, flexible, and robust model for nearshore wind-wave growth and propagation. Recent upgrades to the model include wave-current interaction and steepness-induced wave breaking. STWAVE is written by the U.S. Army Corps of Engineers Waterways Experiment Station (USACE-WES). The method of analysis used by the STWAVE code along with the file formats and input parameters are described in the STWAVE documentation.


## Software Evaluation Criteria

The following criteria can be applied to somewhat quantitatively gauge the extent to which a given piece of software conforms to modern software practices, including the desired properties mentioned earlier. These criteria have been chosen largely based on past ERDC software development bottlenecks encountered in a significant number of projects. These bottlenecks have at times caused development delays, excessive funding additions and in some of the worse cases, an inability to meet final deliverables. Software lacking in one or more of these criteria may increase project risk and overall turn-around time.

**Linkable**: This criteria asks if the given software is configured in such a way as to be linked with and executed by external software. For this to be possible, the software must have three callable phases of execution:  

1. Initialization – a callable routine that initializes the software. Oftentimes this means reading in suitable input files, memory allocation, etc.  
2. Run – a callable routine that executes the actual solution procedure of the software. For example, this routine might be responsible for integrating a partial differential equation.  
3. Finalization – a callable routine that finalizes the software. This could mean closing all open files, deallocating memory, etc.

Note that ESMF (Earth, System Modeling Framework) compliancy is often used as a synonym for the “linkable” trait. It is not called that here because ESMF compliancy has evolved to require much more than the three phase software design given above.

**Distributed**: This criteria asks if the given software can be executed on a High Performance Computing (HPC) machines in a distributed way. This is typically done by parallelizing the code via hard-wiring Message Passing Interface (MPI) calls into the software. MPI is a message passing library standard that is based on the consensus of the MPI Forum, which has over 40 participating organizations, including vendors, researchers, software library developers, and users. The goal of the Message Passing Interface is to establish a portable, efficient, and flexible standard for message passing that will be widely used for writing message passing programs. The advantages of developing message-passing software using MPI are portability, efficiency, and flexibility. MPI is not an IEEE or ISO standard, but has in fact, become the "industry standard" for writing message passing programs on HPC platforms. MPI libraries are also available for use on local multi-processor workstations (including Windows, Mac and Linux operating systems).

**Local**: This criteria asks if the given software relies on local variables instead of global ones. Global variables are, in general, bad software practice for the following reasons:  

- *Non-locality* - Source code is easiest to understand when the scope of its individual elements is limited. Global variables can be read or modified by any part of the program, making it difficult to remember or reason about every possible use.  
- *No Access Control or Constraint Checking* - A global variable can be accessed (get) or set by any part of the program, and any rules regarding its use can be easily broken or forgotten. (In other words, get/set accessors are generally preferable over direct data access, and this is even more so for global data.) By extension, the lack of access control greatly hinders achieving software security in situations where you may wish to run untrusted code (such as working with 3rd party plugins).  
- *Implicit coupling* - A program with many global variables often has tight couplings between some of those variables, and couplings between variables and functions. Grouping coupled items into cohesive units usually leads to better programs.  
- Concurrency issues -- If global variables can be accessed by multiple threads of execution, synchronization of that access is necessary (and too-often neglected). When dynamically linking modules with global variables, the composed system might not be thread-safe even if the two independent modules tested in dozens of different contexts were safe.  
- *Namespace pollution* - Global variable names are available everywhere in a program. When that is the case, a programmer may unknowingly end up using a global variable but think they are using a local variable. This frequently can occur by misspelling or forgetting to declare the local variable or vice versa. Also when linking together different modules that have the same global variable names, if one is lucky, one will get linking errors. If one is unlucky, the linker will simply treat all uses of the same name as the same object. This same issue can also arise with file access.  
- *Memory allocation issues* - Some environments have memory allocation schemes that make allocation of global variables tricky. This is especially true in languages where "constructors" have side-effects other than allocation (because, in that case, you can express unsafe situations where two global variables mutually depend on one another). Also, when dynamically linking modules, it can be unclear whether different libraries have their own instances of global variables or whether the global variables are shared.  
- *Testing and Confinement* - Source that utilizes global variables is somewhat more difficult to test because one cannot readily set up a 'clean' environment between runs. More generally, source that utilizes global services of any sort (e.g. reading and writing files or databases) that aren't explicitly provided to that source is difficult to test for the same reason. For communicating systems, the ability to test system invariants may require running more than one 'copy' of a system simultaneously, which is greatly hindered by any use of shared services - including global memory - that are not provided for sharing as part of the test.

The next 4 criteria refer to possible process level modules/libraries that are being considered as part of the CBSD and which will be described in more detail later in this document. They are being mentioned now, in order to show how each library could be used in several of the numerical models for which CHL makes routine use. Those models are listed in Table 1, along with an assessment of the current status of each model in terms of the component based criteria listed above, as well as the applicability of these potential component based libraries listed next. In Table 1, an “N/A” is used if this library is not appropriate for this software.  

- *MetLib* – This criterion asks if the given software is linkable to the Metlib static library for meteorological inputs.  
- *TransLib* – This criterion asks if the given software is linkable to the Translib static library for sediment and general transport sourcing.  
- *TurbLib* – This criterion asks if the given software is linkable to the Turlib static library for turbulence calculation.  
- *FrictionLib* – This criterion asks if the given software is linkable to the Frictionlib static library for friction variable calculations.

```{r echo=FALSE}
#Create empty table
TableB01 <- as.data.frame(matrix(NA, nrow = 7, ncol = 8))
colnames(TableB01) <- c("", "Linkable", "Distributed", "Local", "MetLib", "TransLib", "TurbLib", "FrictionLib")

#Specify rows of the table
TableB01[1,] <- c("ADH", "Y", "Y", "Y", "N(1)", "N", "N", "N")
TableB01[2,] <- c("ADCIRC", "Y", "Y", "N", "N(1)", "N", "N", "N")
TableB01[3,] <- c("CMS-FLOW", "N", "N", "Y", "N", "N", "N", "N")
TableB01[4,] <- c("Wave Watch III", "Y", "Y", "N", "N", "N/A", "N/A", "N")
TableB01[5,] <- c("STWAVE", "Y", "Y", "N", "N", "N/A", "N/A", "N")
TableB01[6,] <- c("GSSHA", "Y", "N", "N", "N", "N", "N/A", "N")
TableB01[7,] <- c("FUNWAVE", "N", "Y", "Y", "N", "N", "N/A", "N")

#Send output table rows into a single matrix
rownames(TableB01) <- NULL
knitr::kable(TableB01, caption="Table B1. A list of current operational models and a description of their current status related to several component based modular criteria. An N/A is used if this library is not appropriate for this software. Note that (1) means that the software is compatible with WindLib which is a precursor to MetLib", align="c") 
```


## External Library Linkage Work Plan

The last four criteria involve linking software to external process libraries. These libraries are being proposed to collect and combine common development efforts required by a number of ERDC models. Such libraries provide a development focal point to eliminate funding redundancy among ERDC programs and push for common interfaces. They represent a key component of the CBSD.

**Tasks**: This work plan addresses linkage of an already existing external library to a given piece of software. Although this process varies in complexity depending on the software and library, the following steps must usually be taken:  

1. *Wrapper Construction*: A wrapper file in the native language of the software must be created which links variables through memory to the library. Oftentimes, the wrapper must be capable of linking two different computing languages, involve coordinate transformations, and other spatial and temporal interpolation. (2 weeks 1 FTE)  
2. *Build Modifications*: The software build must be modularized to include the external library when appropriate. The AdH software, for example, utilized CMAKE to present a host of executable build options, easily checkable from a GUI interface. (1 week 1 FTE) *Note that “build” refers to the instructions used by computer language compilers to actually compile and create the software executable program.  
3. *Verification*: Once the wrapper is constructed and the software is capable of being built using the linked library, the linkage must be verified against analytic test cases and compared to previous unlinked versions of the software. (2 weeks 1 FTE)

Although the development time required for software linkage varies depending on whether mixed languages, etc. are present, the complexity of the library/module, and the degree to which the model is already modularized, a conservative estimate for the above steps is four to five weeks for one full time employee (FTE) with knowledge of the software. The FTE should have expertise in computer science, particular in building executables with GNUMake and CMaKe.

**Models Potentially Benefiting:: AdH, AdCirc, CMSflow, WaveWatch III, StWave, GSSHA, Proteus, and FUNWAVE**


## Global Variable Removal Work Plan

Global variables, as the name implies, are variables that are accessible globally, or everywhere throughout a program. Once declared, they remain in memory throughout the runtime of the program. This means that they can be changed by any function at any point and may affect the program as a whole. As mentioned, they are generally considered bad practice for the aforementioned reasons. For example, it is very easy for the programmer to lose track of values of global variables, especially in long programs, leading to bugs that can be very hard to locate. Source code is best understood when the scope of its individual elements are limited, so because of their non-locality, it is hard to keep track of where they have been changed or why they were changed. This is just one example of many as to why global variables should be avoided in modern software design.

**Tasks**: The work plan for removing global variables from a given piece of software can vary rather dramatically depending on the current software framework. Additionally, since software variables are intimately tied to the framework of most codes, a robust testing phase is often required. Generally, the following tasks are needed to localize software variables:  

1. Assess the current state of the software to determine which global variables must be localized and create a software mapping plan for how these variables will be stored as instances and local variables.  
2. Implement the localization.  
3. Conduct extensive verification of the localized software for quality assurance

Recent efforts for removing global variables from the AdH software have shown that it took one full time employee about six months of development and quality assurance to complete the process. The FTE must be a highly technical model developer, preferably proficient with both general computer science practices as well as in the physics of the software. This work can be a significant undertaking and there may be a limited number of people with the required expertise.

**Models Potentially Benefiting:: AdCirc, WaveWatch III, STWAVE, GSSHA, FUNWAVE**


## Software Distribution (HPC) Work Plan

A distributed system consists of a collection of autonomous computers, connected through a network and includes distribution middleware, which enables computers to coordinate their activities and share the resources of the system. In this way a user perceives the distributed system as a single, integrated computing facility. Some advantages of distributed computing include:  

- Highly efficient in terms of time to complete a simulation  
- Scalability  
- Less tolerant to failures  
- High Availability

As mentioned earlier the Message Passing Interface is a standardized and portable message-passing system developed for distributed and parallel computing. MPI provides parallel hardware vendors with a clearly defined base set of routines that can be efficiently implemented. As a result, hardware vendors can build upon this collection of standard low-level routines to create higher-level routines for the distributed-memory communication environment supplied with their parallel machines.

MPI gives a user the flexibility of calling sets of routines from C, C++, Fortran, C#, Java or Python. The advantages of MPI over older message passing libraries are portability (because MPI has been implemented for almost every distributed memory architecture) and speed (because each implementation is in principle optimized for the hardware on which it runs).

**Tasks**: Previous efforts to distribute legacy software using MPI for HPC applications have shown the level of effort involved to be an undertaking which largely depends on the legacy framework and structure of the code itself. More often than not, it can take several months to design a parallel paradigm for a code, implement that scheme, and tune it for better scaling performance. Additionally, since distribution (parallelizing) affects software variables that are intimately tied to the framework of most codes, a robust testing phase is required.

The following 4 tasks are generally needed to complete this work plan:  

1. Assess the state of the legacy software to determine an optimal method for distribution. This should include profiling the code to locate wall clock bottlenecks using commercially available third party software.  
2. Distribute the software, i.e. use MPI code to parallelize.  
3. Verify the newly distributed software using the software’s standard test suite (if available) and comparing the serial and distributed results.  
4. Conduct a final software wall clock profile to ensure that all bottlenecks are addressed

A conservative estimate for MPI distribution of serial software with adequate quality assurance would be 8 – 10 months for one FTE. Again, it is assumed that the FTE must have an extensive background in HPC architectures, the MPI language, and general computer science experience, plus knowledge of the physical/mathematical processes in the code is a major advantage also.

**Models Potentially Benefiting :: GSSHA, CMS-FLOW**

## Process Libraries

This next section describes several possible process libraries that could be created for use by several of the CHL numerical models as part of the component based software design portfolio.

### MetLib (Meteorological Library)

Hydrodynamic modeling is a large industry in today’s engineering world, and as computational resources grow cheaper and faster, application domains grow in size and complexity. For these large scale hydrodynamic, hydraulic and transport problems, spatially varying meteorological input such as atmospheric pressure, wind velocities, precipitation rates, and temperature are required to drive dynamical simulations. These meteorological fields are most often computed a priori by external software and one-way coupled to the hydrodynamic software. More often than not, the meteorological data has a file format that differs from those used by the hydrodynamic software and may even be proprietary to the software used to create it or the instruments that collected and recorded the data. This typically means that a pre-processing step is required to change meteorological data from a given format to ones that are suitable for each hydrodynamic model.

There is a great need within the ERDC modeling community for a generic meteorological input library with distributed (parallel) capabilities for use with hydrodynamic, hydrologic and transport models. This library could be linked to any hydrodynamic model and provide format interpreters and regridding capabilities, thereby eliminating the need for a separate preprocessing step. This would speed up the overall modeling process and reduce the possibility of human error. Furthermore, as the meteorological library is expanded, all the hydrodynamic models that use it immediately benefit.

The Meteorological library proposed would provide atmospheric data by (1) reading in the meteorological data for a large variety of file format options and (2) efficiently interpolating that data both between time-snapshots and, when necessary, between the input meteorological and the hydrodynamic spatial grids, both of which can be structured or unstructured. The library would also contain parametric models such as the Holland vortex model (Holland, 1980) for tropical cyclones.

Trahan et al 2017, unpublished technical report, have initiated development of such a library in their creation of the stand-alone static library WindLib, already routinely linked and used within the AdH model. WindLib leverages and was first created by extracting a subset of wind input capabilities from the ADvanced-CIRCulation shallow water model, ADCIRC. ADCIRC’s atmospheric input options are extensive, providing options for separate meteorological grids (subsequently interpolating onto the hydraulic mesh), time-interpolation between atmospheric snapshots, and numerous data input formats such as the US Navy Fleet Numeric format, Planetary Boundary Layer Hurricane Model format, NOAA’s Geophysical Fluid Dynamics Lab’s (GFDL) format, etc. Additionally, the ADCIRC model has several parametric wind models, such as the Holland vortex model, coded. To build WindLib from ADCIRC, pertinent routines were extracted and a parallel wrapper developed for use with AdH. For verification of WindLib portability, the library was linked to both AdH and ADCIRC, and the linkage verified against the current wind test suite for each model. WindLib can be used by different hydraulic suites, instead of manually adapting the data to a specific piece of software.

**Tasks**: Development of MetLib will leverage the previous WindLib work and extend it to include the full suite of ADCIRC meteorological options and also those used by other hydrodynamic and transport models. The effort can be broken down into four major tasks:  

1. Extension of the WindLib static library to include new meteorological variables by either extraction from existing software or creation of new routines, all leveraging the existing work of WindLib.  
2. Creation of a Makefile that can build machine portable static libraries for linking to a variety of hydrodynamic models.  
3. Verification of the new meteorological variables from MetLib using software such as AdH.

Note that these tasks are only for MetLib creation and verification and do not include efforts required to link a given hydrodynamic model to MetLib. AdH is mentioned in verification since it has already been linked to Windlib, the precursor to MetLib. In order to link MetLib to a general hydrodynamic model, a wrapper must be created specific to that model and the static library added to the executable build instructions and build process.

**Models Potentially Benefiting :: AdH, AdCirc, GSSHA, Proteus, CMS Flow, StWave, WaveWatch III, FUNWAVE (Future Use and R&D)**

### TransLib (Transport Library)

Pivotal to the ERDC environment quality mission is the need to simulate species/constituent transport in hydrodynamic systems. This is usually done by solving either (1) discrete particle trajectory Lagrangian equations or (2) continuous transport equations. Particle transport models can be used for a wide variety of projects, including modeling sediment, species and water quality dynamics. Although just about anything can be transported, particle-specific contributions can be condensed down to providing a mother transport equation with a few common items, advection corrections, sources/sinks, for example. There is a need for a common interface library that collects ERDC particle-specific routines for linking to both Lagrangian and continuous particle-based solvers. Importantly, TransLib, itself, will not solve the transport equation in any form. The library only provides a mother solver with species/constituent information.

The following ERDC codes have particle processes that should be considered as inclusion into the TransLib:

**CORSED**: A fundamental component of the TransLib library will be sediment. Sediment transport is pivotal to the ERDC environmental quality-modeling mission. Currently, there exist two separate sediment transport libraries, SedLib and SEDZLJ. Current R&D efforts are in place to combine these libraries into a more general one named CORSED. The TransLib library will take this one step further and wrap CORSED into a general ERDC transport library. This library would then be capable of supplying to linked hydrodynamic models, corrections to sediment advection as well as supplying source and sink terms.

**PT123**: PT123 is an ERDC particle tracking model designed to track massless particles in 1-, 2-, and 3-D unstructured or converted structured meshes. One adaptive (embedded 4th- and 5th-order) and three non-adaptive (1st-, 2nd-, and 4th-order) Runge-Kutta (RK) methods are included in PT123 to solve the ordinary differential equations describing the motion of particles. Both element-by-element (EBE) and non-element-by-element (NEBE) tracking approaches are incorporated into PT123. Recently, oyster physics was added into PT123. Oysters are a good representation of the ecosystem quality in estuaries, and are thus modeled quite a bit within the ERDC. The PT123 oyster processes will be extracted and added into TransLib.

**PTM**: The Particle Tracking Model (PTM) is a Lagrangian particle tracker designed to allow the user to simulate particle transport processes. PTM has been developed for application to dredging and coastal projects, including dredged material dispersion and fate, sediment pathway and fate and constituent transport. The PTM has a host of species and water quality processes that can be extracted into the TransLib.

**Tasks**: Development of TransLib will leverage the previous CORSED work and extend it to include species and water quality processes in PT123 and PTM. The effort can be broken down into the following tasks:  

1. Create a wrapper over the CORSED library to include it into TransLib (2 weeks)  
2. Extract PT123 and PTM water quality and species process routines and implement them into the TransLIb library (4 weeks)  
3. Create the TransLib static library (1 week)  
4. Verify Translib components (SedLib, SEDZLJ, PT123, PTM) (4 weeks)  
5. Create TransLib user documentation (1 week)

Note that these tasks are only for TransLib creation and verification and do not include efforts required to link a given hydrodynamic model to the library. In order to link TransLib to a general hydrodynamic model, a wrapper must be created specific to that model and the static library added to the executable build (see previous work plan).

This work requires 1 FTE for development, but the coordination of many ERDC software teams to implement. The developer should have expertise in computer science, software design, particle tracking methods and hydrodynamics.

**Models Potentially Benefiting :: PT123, PTM, AdH, ADCIRC, CMS-Flow**

### TurbLib (Turbulence Library)

When Reynolds averaged models are utilized, a turbulence model is needed to estimate the Reynolds stresses in the governing equation. Most often, these stresses are defined in terms of known averaged quantities. There are a number of ways to do this, for example, by using transport or non-linear eddy viscosity models. Turbulence closure models/equations are used to account for processes that arise as a result of chaotic changes and/or gradients in pressure and velocity. These changes increase the hydraulic/hydrodynamic losses in the flow field, most often through the generation of vortices that may or may not interact with each other.

Currently, the AdH software suite has a hard-wired turbulence library that supports a host of turbulence models and options. These files can be leveraged to create a stand-alone library for linking to other hydrodynamic models. Within the AdH model, turbulence routines are written in a generic length scale format. This convenient set-up only requires that the linked model send coefficients that match the different turbulence options.  

Current AdH turbulence options include:  

- Horizontal Eddy Viscosity and Diffusivity (sub-grid scale models)  
    - Smagorinsky Formulation  
- Vertical Eddy Viscosity and Diffusivity (Boussinesq hypothesis models)  
    - Mellor-Yamada Level 2  
    - Mellor-Yamada Level 2.5
    - Kappa-epsilon
- Buoyancy Supression Options
    - Henderson-Sellers  
    - Munk-Anderson  
    - Kent-Pritchard
    - Pritchard
    - French-McCutcheon

Another option being consider by the team is whether instead of creating TurbLib from scratch, could the existing community developed General Ocean Turbulence Model (GOTM) be used instead, or could AdH’s routines be used along with the GOTM routine to provide a much wider range of options. GOTM has been being developed for over 20 years and has an extensive user base. For more details, see the GOTM website (https://gotm.net/).

**Tasks**: This work plan can be broken down into three stages:  

1. Extraction of the current AdH turbulence library so that it is buildable as a stand-alone static library for linking to other software. (2 weeks)  
2. Testing the new turbulence library by linking to AdH and comparing results to previously hard-wired turbulence options. (5 weeks)  
3. Testing linkage of GOTM to AdH. A comparison of results from ADH compared to its original turbulence options (where appropriate) as well as a comparisons of computation performance using GOTM. (5 weeks)  
4. Final recommendation on using AdH’s turbulence options, the GOTM library, or a combination of the two for TurbLib to be used by other hydrodynamic models. (1 week)  
5. User manual creation. (1 week)

This effort requires one FTE skilled in turbulence theory, software development and basic fluid dynamics.

**Models Potentially Benefiting :: AdH, AdCirc, CMSflow, Proteus**

### FricLib (Friction Library)

Generally, hydrodynamic models resort to the application of some sort of drag law near the -bed to represent the turbulent frictional processes there. The use of lateral (i.e. horizontal) friction, on the other hand, is an almost universal constraint imposed on all hydrodynamic numerical models by their inability to follow the nonlinear cascade of energy below scales of the mesh size. This friction effectively prevents the pileup of energy in a numerical simulation in the wavelength range corresponding to the mesh size.

Hydrodynamic models, in general, apply frictional effects as a stress in the momentum equations. Empiricism is imparted to this frictional stress term through the usage of a “coefficient of drag” or “coefficient of friction”. This coefficient is dependent upon characteristics of the bed boundary layer. The library proposed here will supply a variety of drag coefficients to a linked hydrodynamic/wave model. The library can be built by leveraging current friction routines within the Adaptive Hydraulics suite. AdH currently has the ability to calculate a number of friction coefficients for hydrodynamics equations using physically relevant formulations to accurately account for friction losses by calling a friction library of routines for coefficient calculation and/or conversion. A few of the most widely used friction library calculations will now be described.

**Bed Shear Coefficient of Friction** - computes a shear stress coefficient resulting from a steady (or quasi-steady) current field. The formulation given here is derived from a modified form of the classic logarithmic velocity profile. The traditional profile yields a velocity of infinity at the bed; whereas the modified profile forces the velocity to vanish at the bed.

**Submerged Aquatic Vegetation Coefficient of Friction** - computes a shear stress coefficient resulting from a steady (or quasi-steady) current field over a bed consisting of submerged aquatic vegetation (SAV).

**Unsubmerged Rigid Vegetation Coefficient of Friction** - computes a shear stress coefficient resulting from a steady (or quasi-steady) current through rigid, unsubmerged vegetation. Some examples of this might include flow through mangrove stands, phragmites in coastal wetlands, or trees and other obstructions in coastal storm surge flooding. The formulation includes both the form drag induced by flow through the obstructions and the skin drag induced by flow over the bed.

**Evenly Distributed Obstruction Coefficient of Friction** - computes a shear stress coefficient for use in computing the shear stress resulting from a steady (or quasi-steady) current through or over an evenly distributed field of flow obstructions. This formulation can be used to simulate flow through or over wetland vegetation, trees, buildings or any other subgrid-scale obstructions. The obstructions are modeled as a field of evenly distributed cylinders.

**Tasks**: This work plan can be broken down into three stages:  

1. Extraction of the current AdH friction library so that it is buildable as a stand-alone static library for linking to other software. (2 weeks)  
2. Testing the new friction library by linking to AdH and comparing results to previously hard-wired turbulence options. (5 weeks)  
3. User manual creation. (1 week)

This effort requires one FTE skilled in finite element methods, software development and basic fluid dynamics.

**Models Potentially Benefiting :: AdH, AdCirc, CMSflow, Proteus, FUNWAVE, STWAVE, and WaveWatch III**

