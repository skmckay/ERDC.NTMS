[["index.html", "Community based Numerical Technology Modernization Strategy Overview Mission Goals Glossary and Acronymns Points of Contact", " Community based Numerical Technology Modernization Strategy ERDC Coastal &amp; Hydraulics Laboratory 2019 Overview RECREATION OF 2019 DOCUMENT AS EXAMPLE. Could include example of version control in documentation format. Major.Minor.Patch becomes Publication.SectionRevision.Typo? Insert hyperlink to original pdf document. Mission The mission of the Coastal and Hydraulics Laboratory (CHL) is to deliver solutions to our Nation’s most challenging coastal and hydraulic problems through research, development and application of cutting-edge science, engineering and technology. To support this mission, CHL is adopting this Numerical Technology Modernization Strategy (NTMS) to revolutionize the way it develops, applies, transitions into practice, and maintains numerical technologies developed for Civil Works and Military applications. The goals of the strategy are to guide CHL’s investments in numerical and data technologies; to improve numerical modeling and data integration infrastructure; to establish a common system for digital technology development and maintenance, considering the entire business life cycle (from development to retirement) of CHL’s software and data products; to improve and integrate relevant data and numerical technologies, from simple tools and calculators to expert modeling systems, so that they are efficient to access, apply, and cost less to use and maintain; and to document a plan to facilitate interagency collaboration as well as for leveraging and growing resources. Goals Guide investments in numerical and data technologies. Improve numerical modeling and data integration infrastructure. Establish a common system for digital technology development and maintenance. Improve and integrate relevant data and numerical technologies. Make data and numerical technologies efficient to access and apply while costing less to use and maintain. Document the vision to foster ERDC, interagency, and academic collaboration and to leverage resources Glossary and Acronymns ADCIRC: “a system of computer programs for solving time dependent, free surface circulation and transport problems in two and three dimensions. These programs utilize the finite element method in space allowing the use of highly flexible, unstructured grids.” For this study, ADCIRC was used to predict storm surge. AdH: Adaptive Hydraulics (AdH) numerical code is “a modular, parallel, adaptive finite-element model for one-, two- and three-dimensional (2D, and 3D) flow and transport.” For this study, AdH was used to predict changes in local hydrodynamics. CF: Climate Forecast. CHL: ERDC’s Coastal &amp; Hydraulics Laboratory. CHS: Coastal Hazards System. DOD: Department of Defense. ERDC: U.S. Army Engineer Research and Development Center. ESMF: Earth System Modeling Framework. FRF: CHL’s Field Research Facility. GOTM: General Ocean Turbulence Model. GUI: Graphical user interface. HPC: High Performance Computing. IT: Information Technology. NetCDF: Network Common Data Form. NOAA: National Oceanic and Atmospheric Administration. NTMS: Numerical Technology Modernization Strategy. PETSc: Portable, Extensible Toolkit for Scientific Computation. USACE: U.S. Army Corps of Engineers. USBR: United States Bureau of Reclamation. USGS: United States Geological Survey. VVUQ: Verification, validation, and uncertainty quantification. Points of Contact Chris Massey, Ph.D., Coastal &amp; Hydraulics Laboratory, U.S. Army Engineer Research and Development Center, Vicksburg, Mississippi Email: Chris.Massey@erdc.dren.mil Tahirih Lackey, Ph.D., Coastal &amp; Hydraulics Laboratory, U.S. Army Engineer Research and Development Center, Vicksburg, Mississippi Email: Tahirih.C.Lackey@erdc.dren.mil "],["numerical-technology-modernization-strategy.html", "Chapter 1 Numerical Technology Modernization Strategy 1.1 Introduction 1.2 Reinvented solution workspace 1.3 Unified data and metadata standards 1.4 Verification, validation and uncertainty quantification 1.5 Component based software 1.6 Community partners 1.7 References", " Chapter 1 Numerical Technology Modernization Strategy Version: 1.0.0 Date: 2019 Written by: Chris Massey (Chris.Massey@usace.army.mil) 1.1 Introduction This is a living document, intended for continuous review and updating, at least annually, as progress is made on the implementation of the NTMS and as new technologies emerge. While the NTMS started within CHL, success in implementing the strategy will only be realized with the participation of other ERDC labs, USACE districts, as well as partner federal agencies. The numerical technology modernization strategy strengthens the entire coastal and hydraulics enterprise by equipping practitioners with the best numerical tools available. In so doing, it will perpetuate CHL as a continued leader in pioneering coastal and hydraulic numerical technology research and development and will integrate expertise and resources across business, program, and organizational lines. Numerical models are increasingly seen as a commodity, particularly in an ever-increasing open source development culture, and this strategy will guide CHL within this culture, to relentlessly pursue collaborations in development of leading edge technologies. CHL will work alongside our partners to transition state-of-the-art to state-of-the-practice through enhanced customer support and technology transfer efforts. In this document, the word “technologies” is used to encompass models, tools, data and information technology (IT) infrastructure. ”Models” refers to numerical computer algorithms, e.g. computer software that solves systems of equations. “Tools” refers to mechanisms that access, create, and/or manipulate data, such as graphical users interfaces (GUIs) and cloud/web-based calculators. “Information Technology (IT) infrastructure” refers to computing hardware systems, data storage systems and internet access/bandwidth. Support software and project management software, such as “git” for version control and “Jira” for project management software, overlaps both Tools and IT infrastructure. The three pillars of the NTMS are component-based community software; seamlessly integrated data and models; and cloud/web-based infrastructure/tools. These components are reinforced and interconnected with an integrated verification, validation and uncertainty quantification (VVUQ) process. The VVUQ process continually ensures the integrity of the components and indicates directions for needed research and development improvements. The strategy seeks to layout the necessary foundations for integrating the three pillars to achieve a reinvented solution workspace that has both process driven computational capabilities and data that are seamlessly interchanged and are cloud environment enabled. The workspace refers to more than the tools, e.g. interfaces, and data, it also encompasses the workflow processes as well as the access to and resources upon which everything is executed and stored, e.g. infrastructure. The seamless interchange between models, tools, and data requires that all are readily accessible across operating platform environments, which is particularly relevant in light of big data analytics. The user experience will evolve into a model agnostic interface that is customizable, yet familiar and is integrated with best practices and USACE guidance documents along with training and user manuals. In order to be accepted and trusted by the end users, these technologies must first be accurate and robust, have appropriate user support and training, and be well validated and verified for the problems they are solving. The NTMS can be broken down into five major action areas: Teams have been formed for each of the first four major areas in order to further flesh out details and needs, and then make recommendations and resource estimates for implementing the strategy in a series of plans. Community partners will be engaged to expand the strategy and to help formulate plans for implementing it. The five action areas are described in the remaining sections of this document. 1.2 Reinvented solution workspace The purpose of having a reinvented solution workspace is to transform the current models, tools, e.g. interfaces, and workflows into a common environment that operates on anticipated future computational resources, including cloud environments. Furthermore, it will have a common look and feel to modeling and data processes across the spectrum of modeling, which reduces learning curves. At the same time, the new workspace allows for customizable workflows that are in line with USACE guidance and community best practices. The new workspace is required because how we as a community of practice interact with models and data in the future will be very different than today. Big data, the computational complexity of modeling systems of systems, and the proliferation of portable handheld devices are driving forces moving the community toward cloud-based computing and web/app-based applications. Our tools must be collocated with future computation resources and allow ready access to required data, both inputs and outputs. To create this new paradigm, new tools for model/data interaction are required. These requirements are in part responsible for the need to implement the other components of the NTMS. For example, using unified data and metadata standards for stored data and for use in both complex numerical models and simpler on-line tools, helps to facilitate a model agnostic interface and reduce workflow complexities by removing reformatting. This model agnostic interface is a significant part of the design concept for the new workflow. Table 1.1: Dimensions of a reinvented solution workspace. 1. Common environment (cloud) 2. Seamless model-data interaction 3. Model agnostic interface 4. Customizable workflows 5. Integrated with guidance documents Each of the individual components will have a similar look and feel with a common branding, a unified naming convention that is not tied to an organization. A new common branding for the workspace solutions does not mean that existing names of models/tools will change. To illustrate suppose that WIDGIT is the common branding name, then WIDGIT-Hydro would be the name of the process-based workflow that solves the shallow water equations. That workflow would use existing models like ADCIRC, AdH, and CMS-Flow as the solution engines. Each of those models have varying degrees of fidelity, speed of computation, uncertainty, and complexity of setup and execution. The new processed based workflow WIDGIT-Hydro and the new interface would eliminate the user from having to deal with individual model specific requirements. The new interface must also be configurable and allow individual processes to be linkable by the users in a customizable way. The workspace must have readily available preconfigured coupled workflows that exist as selectable options. These workflows will be informed by and comply with USACE guidance and best practice procedures. With these preexisting workflows, the reinvented solution workspace can have a question and answer process that leads a user through a series of guided questions to determine not only the correct process to use, but also the appropriate “modules” to apply based on specified error tolerances and desired level of effort. Expert users would still have the ability to select specific models for use instead of being guided. For this new workspace to be realized, numerical models and tools will be platform independent in their execution. Specifically, large operational numerical models need flexibility to be compiled and run under native operating systems, such as Windows, Mac OS, and Unix/Linux. The models will have a common file type and data structures to aid in data exchanges. They will be designed or re-designed to be scriptable and callable so that they can be incorporated into larger workflows by coupling and integration with other applications and be executable under or through web-based applications. Each model and tool will be analyzed consistently, fairly, and expertly for accuracy and applicability to target problems to aid the model/data selection process. Furthermore, metrics are required to evaluate the complexity and skill sets needed for setting up and executing the appropriate workflows to solve engineering problems. Having such metrics and measures will facilitate filtering of workflow components for different applications and budgets. These are just a few highlights of the traits and capabilities of the reinvented solution workspace of the future and some of the changes needed to achieve it. A whitepaper is currently being developed, led by a team of experts from CHL with input from users both within USACE districts and from external partners, in order to develop plans for implementation, particularly the workspace interfaces. The whitepaper will also document what has worked well with existing user interfaces, their development process and their user tech support procedures. In this way, best practices can be maintained and carried forward. It is anticipated that external partners will help to advance these new technologies through open-source licenses and peer-reviewed and refereed processes. 1.3 Unified data and metadata standards The purpose of the unified data/metadata standards is first to make files self-describing in content and structure to ensure details about the file contents are known, allowing users to make informed decisions about the suitable use of the data. Secondly, when fully implemented, the standards will facilitate certain types of numerical model coupling, make them easier to use and more transportable. A unified data structure means a single file-type/format and a commonly adopted single method for describing and writing the data in the file. Using a unified data structure will allow each model to be capable of reading and writing a file that is immediately recognizable and usable to other models of similar types. It also means that interfaces and downstream products and applications, like the Coastal Hazards System (CHS), do not require the data to be reformatted prior to use and can have access to many more data sources than are currently available. The use of the Network Common Data Form (NetCDF) file types has been adopted as a common starting point along with the adoption of the Climate-Forecast (CF) Standards for metadata and file structures (CF-Conventions, 2020). The NetCDF4 (Unidata, 2020) library is readily available across multiple computing platforms, which allows for easier porting and exchange of files. It is designed to read and write data in a well-defined and structured way. Various other federal agencies including the National Oceanic and Atmospheric Administration (NOAA) and the United States Geological Survey (USGS) have adopted NetCDF file types. Metadata details all aspects of a data type and a file in such a way that a user or a software suite can understand the contents of the file and have information about when, where, and how the data were produced. Standard naming conventions aid in removing uncertainty in the meaning of a data variable and allows for easier discovery of data within a given file by automated systems. The unified data integration team has recommended that metadata inclusion be broken down into three classifications: Table 1.2: Classifications of metadata inclusion. 1. Required values 2. Referred values 3. Optional values This tiered strategy is aimed at making sure the most important metadata gets included in every file so that data are not rendered useless if it is missing. For example, having calculated solution values from a numerical model, but not knowing their coordinate system, units of measure and quality control ratings can render the data useless, while not knowing the date the model was executed is less critical. The unified data integration team is working on a series of whitepapers to specify a standardized format for data types, e.g., waves, circulation hydrodynamics, sediments, water quality and species, in collaboration with the Environmental Lab, to document standards for metadata inclusion. The documentation for waves is the first set of guidelines to be developed and is currently being refined (August/September 2020). Circulation hydrodynamics is next to be completed in (September/October 2020), and the other process guideline whitepapers will be completed by December of 2020. 1.4 Verification, validation and uncertainty quantification There is an old saying attributed to George Box (1978) “Models, of course, are never true, but fortunately it is only necessary that they be useful.” The purpose of VVUQ is to document model accuracy and uncertainty. Ensuring models are solving what they claim they are solving is paramount in building trust in their application. Verification determines if the computational model fits the mathematical description of the process while validation is performed to determine if the model accurately represents the real world application. Robert O’Keefe and Daniel O’Leary (1993) give a simple yet very useful definition of the two as “Verification refers to building the system right, while validation refers to building the right system.” Uncertainty quantification attempts to quantitatively identify, classify, estimate, and when possible reduce uncertainties in both numerical models and measured/collected data. Philip Stark (Stark, 2017) describes it this way, “UQ tries to appraise and quantify the uncertainty of models of physical systems, which are calibrated to noisy data (and of predictions from those models), including contributions from incomplete knowledge of a process; systematic and stochastic measurement error; limitations of theoretical models; limitations of numerical representations of those models; limitations of the accuracy and reliability of computations, approximations, and algorithms; and human error.“ Since models are continually being developed, improved and otherwise changed, and the same for data collection methods, it is important to have procedures in place that can assess the impacts of changes. For numerical models, it is important to be sure that software changes have not broken the model so that it no longer executes, or worse, executes and produces wrong or unintended results. When new/different instruments are used to collect measured data, it is important to know how the error tolerances (precision of the instrument) in the new systems differ from the old system. As part of the NTMS, automated software regression tools will be implemented along with version control software and repositories to keep track of code changes and generate alerts to changes in software performance. The Verification, Validation and Uncertainty Quantification (VVUQ) team sent out a survey to team leads for all major CHL developed and used numerical models. The survey captured existing VVUQ processes and cases for each of the models. The results of that survey were summarized into a whitepaper in March 2019, and the survey results serve as a jumping off point for developing a detailed and comprehensive set of procedures and metrics for implementing standardized and consistent VVUQ process for all models. These test cases are physics-based and not just model-based. They will consider the full range of VVUQ from analytic to lab and field tests including the Model Testbed work (Bak, 2016) at the Field Research Facility (FRF). Some of the VVUQ test cases can also double as training materials for the process models and will facilitate the automated regression testing capabilities. These automated regression test are typically performed on regular schedules, sometimes nightly, and will be required to be performed and documented as part of any new release versions of codes, both complex models and simpler one-line tools. Furthermore, software developers would apply these tests frequently while making code changes or implementing existing codes on new platforms. 1.5 Component based software The purpose of component-based software is to design and organize numerical codes into process based functions and routines, in order to streamline changes and updates to large numerical codes, as well as to share and take advantage of common process libraries. Whether large or small, having numerical codes organized into single processes or functions makes them easier to debug and update. Furthermore, models should be broken down into three main tasking phases: initialization, run, and finalization. Table 1.3: Dimensions of component based software. 1. Modular code - process based design 2. Use of community software libraries 3. Coupling framework ready Initialization is the startup process in which variables are typically setup and sized properly and other onetime execution events happen. The run phase is the main component or purpose of the model and is one that might get repeated many times in an overall simulation. The finalization phase is where the model resources are deallocated and the model shuts down in a controlled process. The phased approach also aids in the models being called by external models, which is very useful in model coupling and other scriptable procedures. Having models further broken down into process level components can allow for reuse of code and build interchangeable process libraries. For example, making use of the general ocean turbulence model (GOTM) library in three-dimensional circulation models, or matrix solvers packages like the Portable, Extensible Toolkit for Scientific Computation (PETSc), can increase model solution accuracies while decreasing model development times. In the same fashion, development of libraries, or process-based components, allows updates to be implemented quickly into models using them. The component-based software team developed a whitepaper in March 2019 that outlines possible process level modules (sediments, meteorology, turbulence, friction, etc.) that are common to several numerical technologies and that could be developed into standalone libraries that are then called by individual models. The whitepaper also details other important modern software practices such as the ability of models to be linked with other models via a coupling framework like the Earth System Modeling Framework (ESMF) and the Coastal Storm Modeling Systems Coupling Framework. Some numerical models have already implemented portions, but not all, of these practices while others will require substantial code rewrites to become fully compliant with such practices. The whitepaper also details some of those specific requirements. The team will work with individual model leads/developers to outline a comprehensive work plan and detailed resource needs for each model to become fully compliant. 1.6 Community partners While CHL is initiating the first version of the NTMS and beginning to develop plans for implementing portions of the strategy, it is highly desirable that the strategy become a community strategy. Other ERDC labs, USACE districts and centers of expertise are especially important partners. Sister federal organizations such as the National Oceanic and Atmospheric Administration (NOAA), the U.S. Geological Survey (USGS) and the U.S. Bureau of Reclamation (USBR) have overlapping modeling needs with those of CHL and in some cases use the same numerical models. Even if individual models are shared, the workflows and data standards are not the same, nor are the tools accessing them. Development is also not traditionally coordinated. Using open source development, unified data standards, community accepted test cases and following a common strategy to guide current and future developments, not only makes good sense, it makes good use of limited resources. Our federal partners are invited to join in refining the NTMS and in helping to formulate plans for implementation. This is somewhat happening now with the partnership at the National Water Center through the Coastal Coupling Community of Practice and recent meetings about the next generation National Water Model, as well as co-development on both ADCIRC and WaveWatch III models. Other partners could be from the Department of Defense (DoD) to include the Army, Navy and Air Force as well as the DoD’s High Performance Computing (HPC) Modernization Program’s CREATE, which stands for Computational Research and Engineering Acquisition Tools and Environments. The CREATE program is designed to improve the DoD’s acquisition process by developing and deploying advanced computational engineering design tools for acquisition programs within military aircraft design; military ship design; RF antenna designs and their integration with platforms; and military ground vehicle designs. The program has flexible operating platforms on the DoD HPC environment as well as a web-based portal. Currently, CHL and the CREATE program leads are exploring ways to work together. Academic partnerships are also desirable for many reasons. With academia’s help, models and tools can be co-developed allowing government and industry users’ access to updated technologies faster. At the same time, the community based open source model codes, training materials and example cases from government and private sector, could serve as curriculum materials and be taught in university classes. This would mean that new graduates are already versed in the models that they will use on the job. Corporate partnerships are also desirable. Aquaveo Inc. is the main developer of the USACE enterprise level GUI’s for hydrodynamic models that are in use today. They are willing to partner to adapt their technologies to meet the future need of operating on the cloud and use of adaptable/configurable workflows. Their expertise in geospatial graphical interfaces and experience applying the same tools is invaluable. 1.7 References Bak, S., (1996). Computational Model Test Bed (CMTB): Numerical Model Testing and Evaluation at the FRF. Accessed on 8/20/2020, from https://www.erdc.usace.army.mil/Media/Fact-Sheets/Fact-Sheet-Article-View/Article/756547/coastal-model-test-bed-cmtb/. Eaton, Brian, Jonathan Gregory, Bob Drach, Karl Taylor, Steve Hankin, Jon Blower, John Caron, Rich Signell, Phil Bentley, Greg Rappa, Heinke Höck, Alison Pamment, Martin Juckes, Martin Raspaud, Randy Horne, Timothy Whiteaker, David Blodgett, Charlie Zender, Daniel Lee, 2020. “NetCF Climate and Forecast (CF) Metadata Conventions”, v1.8. Accessed on 6/16/2020, from http://cfconventions.org/Data/cf-conventions/cf-conventions-1.8/cf-conventions.pdf. GOTM, “General Ocean Turbulence Model”, accessed online from https://gotm.net/. O’Keefe, R.M., O’Leary, D.E., (1993). Expert system verification and validation: a survey and tutorial. Artif. Intell. Rev. 7, 3–42. PETSc, “Portable, Extensible Toolkit for Scientific Computation”, accessed online from https://www.mcs.anl.gov/petsc/. Stark, Philip, 2017. “Uncertainity Quantification, Lecture 1.” Accessed online on June 1, 2020 from https://www.stat.berkeley.edu/~stark/Seminars/lesDiablerets17-1.pdf. Unidata, 2020. NetCDF4.7.4. Boulder, CO: UCAR/Unidata Program Center. Accessed online on June 13, 2020 from https://www.unidata.ucar.edu/blogs/news/entry/netcdf-4-7-4. "],["verification-validation-and-uncertainty-quantification-teams-whitepaper-for-the-numerical-technology-modernization-plan.html", "Chapter 2 Verification, Validation and Uncertainty Quantification Team’s Whitepaper for the Numerical Technology Modernization Plan 2.1 Introduction 2.2 Approach 2.3 Results 2.4 Recommendations 2.5 References", " Chapter 2 Verification, Validation and Uncertainty Quantification Team’s Whitepaper for the Numerical Technology Modernization Plan Version: 1.0.0 Date: 2019.03.18 Written by: Matthew Farthing and Chris Massey (Chris.Massey@usace.army.mil) 2.1 Introduction 2.1.1 Purpose There is a rich literature on Verification Validation and Uncertainty Quantification (VVUQ) across a range of disciplines from nuclear engineering to aeronautics and operations research. The purpose of this whitepaper is not to cover these topics in detail. For excellent reviews and textbooks we recommend (among others) [Oberkampf and Trucano, 2002, 2008, Oberkampf and Roy, 2010] as well as the documentation and guidance from groups such as the Defense Modeling and Simulation Coordination Office (DMSCO), https://vva.msco.mil, and more recent efforts in reproducible scientific research [Piccolo and Frampton, 2016; Nature 2018 ]. Rather, our goal is to provide a summary of on-going VVUQ practice in a key set of models and to recommend steps for establishing a lab-wide VVUQ process as part of the Coastal and Hydraulics Laboratory (CHL) Numerical Technology Modernization Strategy (NTMS). 2.1.2 Motivation and Background Computational modeling and simulation are essential tools in the U.S. Army Corps of Engineers (USACE) efforts to address the Nation’s water resources challenges. To do this effectively the models developed, and applied, must be credible. That is, users must have confidence that the simulations from a computational model have acceptable levels of uncertainty and error [Oberkampf and Trucano, 2002]. There are many ways to define error and uncertainty. As working terms, we will adopt those provided by the American Institute of Aeronautics and Astronautics (AIAA) in their 1998 “Guide for the verification and validation of computational fluid dynamics simulations” [AIAA, 1998]. Error is a recognizable deficiency in any phase or activity of modeling and simulation that is not due to lack of knowledge. Uncertainty is a potential deficiency in any phase or activity of the modeling process that is due to the lack of knowledge. Roache (1997) refers to quantification of uncertainty as estimation or banding of the numerical error in a numerical solution as it is used for decision making. More recently, one often sees uncertainty divided into two categories: epistemic and aleatory uncertainty. Epistemic uncertainty is considered to be due to insufficient knowledge, and as such, can be controlled or reduced through additional investigation and improved representation/resolution of a particular system. Aleatory uncertainty on the other hand is due to noise or stochasticity inherent in a system. As such, it is considered to be irreducible [Asch et al, 2016]. Here, we will use the term uncertainty to cover both categories unless noted explicitly. The primary tools for assessing the accuracy of a computational model and the simulations it produces are known as Verification and Validation (V&amp;V). Verification is the process of determining that a model implementation accurately represents the developer’s conceptual description of the model and the solution to the model [Oberkampf and Trucano, 2008]. Validation is the process of determining the degree to which a model is an accurate representation of the real world from the perspective of the intended uses of the model [Oberkampf and Trucano, 2008]. Loosely, verification determines whether or not the equations underlying a computational model are being solved correctly, while validation addresses the question of whether or not the correct equations are being solved. We also distinguish validation from calibration. By calibration we mean the process of adjusting numerical or physical modeling parameters in the computational model for the purpose of improving agreement with experimental data [AIAA, 1998]. Finally, the process of accreditation is the official certification that a model or simulation, or federation of models and simulations, and its associated data is acceptable for use for a specific purpose [DMSCO, 2011]. It is also worth noting at this point that in the context of the DoD Modeling and Simulation (M&amp;S) community, the real-world may include composite systems with natural, automated, and human components [DMSCO, 2011]. Here, we will primarily consider validation for the physical subsystems. Verification includes code verification and solution verification. Solution verification determines the numerical accuracy of computational results. It typically involves quantifying the level of discretization error due to approximating temporal and spatial dynamics as well as the error due to resolving linear and nonlinear systems of equations [Oberkampf and Trucano, 2002]. Code verification encompasses determining whether or not the computational model correctly implements numerical algorithms as well as the entire solution process from reading input to archiving solutions, output quantities of interest (QoI), and run statistics. In other words, code verification requires finding and eliminating programming errors (bugs) through proper testing and software development practice. An associated type of error is usage error, which becomes more and more likely with the complexity of a model. Usage error can be controlled through proper documentation and training. Solution verification leans more heavily on techniques from applied mathematics, while code verification requires techniques from software engineering. Both are within the domain of computational science and engineering [Heroux and Willenbring, 2009]. Validation is more challenging than verification, since it requires assessing whether or not a model’s analysis reflects the real world adequately. In fact, determining the level of error resulting from the numerical discretization is a prerequisite. Fundamentally, validation involves comparing computational model results with experimental data. Quantifying the level of error in the experimental data is then also required before the agreement between a model’s conceptual formulation and the physical system can be assessed clearly. Verification and validation are processes in which certain qualities of a model and its simulations are assessed. Given their complexity, computational models cannot be verified theoretically nor proven to be valid. Rather, they can only be shown to be incorrect or disproved [Oberkampf and Trucano, 2002]. The cornerstones of the VVUQ process are then benchmarks and metrics. That is, the VVUQ process is only as good as the problems through which we evaluate the model and the quantitative measures we use to determine model performance. Verification benchmarks are usually analytical solutions or trusted, highly accurate numerical solutions. Analytical solutions are obviously preferable, but they are typically available only in cases of highly simplified geometries, auxiliary conditions, and/or material parameters for the classes of models of interest to the lab. Techniques like the method of manufactured solutions [Katz and Sankaran, 2011] can help extend the range of available solutions, but reference numerical solutions and comparison with other codes are still important sources of benchmarks. Metrics for verification tests include global norms of the solution error, conservation of mass and energy, and/or maintenance of positivity and/or symmetry if appropriate. It is also important to evaluate the performance of the computational model versus expected behavior of the numerical methods and algorithms it applies. For example, establishing that a solution converges at the expected rate as the spatial and/or temporal discretization increases can be a valuable way to build confidence that a code is behaving as intended [Roache, 1997]. As mentioned above, code verification requires tools and techniques from software engineering. The mapping of formal techniques from software engineering to simulation codes for hydrodynamics and other environmental processes is not always straightforward given the fact that we are often expanding the formulations and/or approximations in our models to better understand and solve engineering problems [Heroux and Willenbring, 2009]. However, minimal requirements like source control and documentation, cataloging of errors (bugs), and automated regression testing have become much easier and accessible over the last decade thanks to tools like gitlab, github, and bitbucket, which integrate source control with support for these development components (and more). Validation benchmarks involve comparison of computational results with data collected from experiments. These experiments include not just observations of the system but also estimated values for physical parameters, initial conditions, and boundary conditions needed for the model simulation. Again, estimates for the uncertainty in these values is also essential. The problems of interest to CHL and ERDC often involve a number of complex processes interacting at multiple scales. Validation of these multiphysics/multiscale problems is particularly difficult, and traditional validation experiments may only be available at the lab or “bench” scale. These experiments often involve not just a reduction in scale but also in the processes that are modeled in order to diminish the number of parameters that are unknown or poorly known. This is akin to the process of decomposing a problem into a hierarchy of phases or subsystems that is recommended by among others [AIAA, 1998]. Validation experiments at larger system scale are certainly possible and are a strength of ERDC in some areas (e.g., through the Field Research Facility at Duck). We will refer to these as field-scale validation experiments. In our view, the essence of a validation experiment is that it is designed with the intention of validating computational models in mind, the necessary physical parameters and auxiliary conditions are known and the primary sources of uncertainty are accounted for and quantified [Oberkampf and Trucano, 2008]. This is obviously a fluid definition that will depend on the physical system, the model, and its intended uses. Generally, one can define Strong Sense Benchmarks (SSBs) as test problems that have the following four characteristics [Oberkampf and Trucano, 2002]: the purpose of the benchmark is clearly understood, the definition and description of the benchmark is precisely stated, specific requirements are stated for how comparisons are to be made with the results of the benchmark, and acceptance criteria for comparison with the benchmark are defined. The notion of an SSB can apply both to verification and validation test problems. Detailed recommendations for SSBs can be found in [Oberkampf, 2008]. Finally, it is important that benchmarks and VVUQ test suites in general be well-documented and widely promulgated. If they are not well understoodd and easy to use, they are of limited value. 2.2 Approach VVUQ is a process that must continue as long as computational models are being developed and applied. While some steps are straightforward, it is not simple or quick for models as complex as those used by CHL and ERDC. As pointed out in [Oberkampf and Trucano 2008, DMSCO, 2011], the limiting factors for VVUQ are typically time and or money rather than completeness of the analyses. As such, it is important to determine where our computational models are currently in the VVUQ process and what resources they have for VVUQ. To begin this assessment, the VVUQ team developed a survey of VVUQ activity (Table A1). For the first round of assessment, the survey was disseminated to key models in the Flood Risk Management Research Program. Table 2.1: Table A1. Survey questions regarding VVUQ. 1. Name, Lab, contact info. 2. Name of the model. 3. Brief description of the model functioning. 4. What verification test problems are available? How are they documented? 5. What validation test problems are available? How are they documented? 6. Are any of the existing V&amp;V test cases included in a form of regular testing frameworks? If so, what existing framework/tool/software is being used? 7. Does the code include a suite of tests for identifying software bugs/errors (i.e., regressions)? If you are using a third party tool for this, please identify that tool. 8. What level of sensitivity analysis and/or uncertainty quantification is integrated in the software/technology suite? 9. Are VV and UQ an integral part of current model execution? For example, do users receive some quantification of uncertainty as part of the model output? Are explicit guidelines and procedures for sensitivity analysis and uncertainty quantification provided for users of the software? Are these provided in technical reports and/or code documentation? 10. Are the VVUQ test suites made available to users? If so, how? 11. What are the estimated costs (dollars and person hours) for developing the VVUQ suite? 12. How much are you currently expending in terms of funds and manpower to support VVUQ? Is VVUQ explicitly supported in any of your funding? 13. What is the estimated cost of continuing your current VVUQ efforts? How much capacity exists in terms of expertise and availability of personnel for supporting additional VVUQ efforts? Are you currently doing VVUQ via outside contracts, or PETTT projects, community based, or something else? 14. What are the biggest gaps in the VVUQ suite? What do you see as the highest priority needs for your particular code and the lab in general? 15. What mechanism do you recommend for disseminating test databases and documentation? 16. Do you have suggestions for the stewardship and governance of the lab’s VVUQ suites? 17. What other suggestions do you have for the VVUQ effort? 2.3 Results Of the sixteen surveys requested, responses were received from ten model teams: ADCIRC, AutoRoute and the Streamflow Prediction Tool (SPT), AdH, FUNWAVE, GSMB, GSSHA, HEC-RAS, HEC-FDA, HEC-WAT, and WaveWatch3. The remaining teams will be asked to submit their responses at a later date. Below we summarize the responses collectively to identify common trends and notable exceptions. Full responses are included in the appendix. The level of VVUQ activities varied widely across the models. In some cases, verification and validation are limited to a handful of benchmarks that are manually administered with minimal documentation. Most models have verification and validation through published reports and user documentation. Fewer include VV benchmarks in an automated testing framework, and still fewer release benchmark test input and data with their software. Several models include some form of automated unit and regression testing. However, access or use of a full development pipeline with automated testing was identified as a key gap by several models. The uncertainty quantification aspect of VVUQ appears to lag significantly behind verification and validation. Some models like HEC-WAT and HEC-FDA have uncertainty quantification as a part of their primary design. Others, like HEC-RAS and GSSHA include sensitivity analysis and Monte-Carlo capabilities for addressing parameter uncertainty, but fully integrated UQ is limited as a rule. Guidance for addressing uncertainty is generally not provided formally or available through user manuals and published technical reports. There was consensus that the best way to disseminate VVUQ benchmarks and information was through web-sites or software portals together with the released code. Right now, most models do not provide benchmark input/output files directly to users via the web. Collectively among the responding model groups, well over a million dollars has already been spent developing the existing VVUQ capabilities. The estimated cost to maintain existing VVUQ efforts or increase them to an acceptable level ranged from roughly 30K to 100K per year, with most models estimating around 50K per year. This estimate seems high and will likely drop once the initial test cases have been created and automatic regression testing software and other automated processes put into place. Likely, the annual cost per model will be on the order of $10k. There was also general agreement that VVUQ benchmarks are needed and could be a useful way of collaborating among modeling groups. Model-specific input files should be provided, but it was agreed that benchmark problems would be best distributed through model agnostic descriptions with model independent data. The gaps identified by the modeling teams were: availability of high-quality data for validation, available personnel with VVUQ expertise, and clearly defined policy and tools for automated testing along with tools for user interface testing and engaging users in testing procedures. In summary, the state of VVUQ in the surveyed models is a mixed bag. The need for VVUQ is recognized universally, although some groups have made more progress than others. There is agreement that cross-model VVUQ efforts are needed, both to provide generality and broader coverage, while also encouraging greater collaboration. 2.4 Recommendations Many recommendations for best practices in VVUQ can be found in the literature. The work of Oberkampf and collaborators is notable [Oberkampf and Trucano, 2008, Oberkampf and Roy 2010], but there is a rich literature spanning several fields. In a military context, guidelines and templates for VV and accreditation are available from the DMSCO, while guidance on software development practices can also be found in recent efforts to bring improved software engineering practice to scientific computing and groups advocating practices for reproducible science [Nature, 2018]. Based on the survey responses and discussions among the NTMS’s VVUQ focus group, we make the following recommendations for next steps for the VVUQ component of the NTMS. First, a standardized set of verification and validation benchmarks are needed. These tests should be grouped by physical process and dynamical regime rather than by model. Designing the benchmarks should be done as a collaboration among model developers, users, and experimentalists and should follow recognized best practices [e.g., Oberkampf and Trocano, 2008]. In particular, the benchmarks must (1) be well-documented in a model agnostic way, (2) have clearly defined metrics for code evaluation, and (3) widely shared via a web-based platform. We also recommend that individual models provide input decks and expected output results in connection with the benchmarks as well as a document describing any particular applicability issues for the given model. Current efforts within the NTMS in developing unified data formats should be leveraged in determining the best format for distributing benchmark tests. VVUQ practice has been driven to this point by individual model development groups. While these teams should obviously still be central in the VVUQ process, shared resources and direction are needed from the lab and technical programs. This includes manpower and funding resources. Resources need to be allocated to the model development teams. However, for VVUQ to be a uniform practice across the lab a small group with identified responsibility for VVUQ practice is likely needed. This group would also require adequate resources and expertise. Furthermore, this group should in general be made up of team members separate from the model develop teams. This separate grouping of testers and developers helps to ensure the integrity of the overall process and aids in the accreditation step to be discussed later. Concrete steps could include identifying a VVUQ focused work unit in the technical programs, requiring VVUQ to be addressed in all modeling related PMPs, and targeting hires in the field of VVUQ. Given funding and manpower constraints, it makes sense to pick an initial set of processes and models to evaluate. A logical choice in our opinion would be to consider wave and circulation models. First, these models are central to much of CHL’s civil works and military missions. Second, the Coastal Model Testbed effort already provides an excellent opportunity to engage modelers, experimentalists, and scientists with experts in field observations. Finally, we have spent relatively little time discussing accreditation. However, this is an essential step for models to be considered fully mature and is now required for certain classes of applications. The DMSCO provides guidance and examples of accreditation (or certification) procedures in the DoD Modeling and Simulation community. Some of these procedures may not be a perfect fit for the classes of models developed and supported by the lab, and there exist other certification procedures more appropriate for civil works applications which some of the surveyed models have already undergone (and continue to undergo). Accreditation of users for models is also important, maybe more so than an actual model accreditation. Expert tools in the hands of untrained users can be very dangerous. It is recommend that CHL develop a series of user certification classes for its models. These classes would be offered to USACE districts, military users as well as made available to academic institutions and external government and private sector contractors. It is recommended that software, infrastructure and suitable training be put in place for automatic regression testing and feature/bug tracking. Some modeling groups are already using some forms of automated regression testing, such as the AdH development team. For issue tracking and feature tasks, applications like Jira or Gitlab’s internal issue tracker can be used. Automated regression testing software such as CTest, from Kitware, or others readily available through the Defense Intelligence Information Enterprise (DI2E). This is an area with the Information Technology Lab (ITL) or the DoD High Performance Computing Modernization Program (HPCMP) could help to make recommendations for software to use, help to set up use cases and possibly provide training. Finally, the next phase is to develop a comprehensive plan for implementing these recommendations. This plan will need to have cost and times associated with each recommendation along with an order of priority for implementation for specific operational models. 2.5 References Asch, M., M. Bocquet, and M. Nodet, Data Assimilation: Methods, Algorithms, and Applications, SIAM, 2016. Challenges in Irreproducible Research, Special Issue, Nature 2018. Defense Modeling and Simulation Coordination Office (DMSCO), website https://vva.msco.mil, published 2011. Katz, A. and V. Sankaran, Mesh Quality Effects on the Accuracy of CFD solutions on Unstructured Meshes, Journal of Computational Physics, Volume 230, Issue 20 pages 7670-7686, 2011. Oberkampf, W.L. and C.J. Roy, Verification and Validation in Scientific Computings, Cambridge Scientific, 2010. Oberkampf, W.L. and T.G. Trucano, Verification and Validation in Computational Fluid Dynamics, Progress in Aerospace Sciences, Volume 38, pages 209-272, 2002. Oberkampf, W.L. and T.G. Trucano, Verification and Validation Benchmarks, Nuclear Engineering and Design, Volume 238, Issue 3, pages 716-743, 2008. Piccolo, S.R. and M.B. Frampton, Tools and techniques for computational reproducibility, Gigascience, Volume 5, doi 10.1186/s13742-016-0135-4 2016. Roache, P.J., Quantification of uncertainty in computational fluid dynamics, Annual Review of Fluid Mechanics, pages. 126–160 "],["component-based-software-design-teams-whitepaper-for-the-numerical-technology-modernization-program.html", "Chapter 3 Component-Based Software Design Team’s Whitepaper for the Numerical Technology Modernization Program 3.1 Introduction 3.2 Software Evaluation Criteria 3.3 External Library Linkage Work Plan 3.4 Global Variable Removal Work Plan 3.5 Software Distribution (HPC) Work Plan 3.6 Process Libraries", " Chapter 3 Component-Based Software Design Team’s Whitepaper for the Numerical Technology Modernization Program Version: 1.0.0 Date: 2019.03.18 Written by: Corey Trahan, Lucas Pettey, and Chris Massey (Chris.Massey@usace.army.mil) 3.1 Introduction The Component-Based Software Design Team was created as part of the overall Numerical Technology Modernization Program (NTMS). The NTMS was designed to improve and integrate the Coastal and Hydraulics Lab’s (CHL) relevant numerical technologies, which in turn produces more robust models and field data that are efficient to access, apply and cost less to use and maintain. Part of the NTMS strategy is for a team to be formed in order to make recommendations on how best to migrate existing software to a Component-Based Software Design (CBSD). The CBSD Team was tasked to emphasize modularity of code through the separation of functionality available throughout software systems. It is a reuse-based approach to defining and implementing independent components that can be coupled to meet various needs. The strategy requires modular components that must be substitutable, enabling components to be replaced with either an updated version or an alternative without breaking the system in which the component operates. Programmers will design and implement software components to be reusable and extensible. The ERDC transition to component-based software development is a long-term strategy and one that will require dedicated effort and funding. Several tactical steps have already been initiated that enable code-level compartmentalization and coupling. These initial steps will eventually lead to process level functional components and coupling abilities at the process level for even finer granularity. Making the transition from our current numerical model practices will require significant software re-engineering of existing codes and transitioning current and future development processes to make use of CBSD. The CBSD plan improves efficiency and reduces time from R&amp;D to solution for new physical theories and computational methods by highly modularizing tools creating flexible computational environments to streamline this process. This task covers design and implementation of multiple layers of Application Programming Interfaces (API’s) to support novel coupling of existing computational models and rapid development for complex, multi-physics applications. The following steps have been identified as critical to successful implementation of this overall plan: Modularize all required software at the model level beginning with an inventory of all models, software, codes and their requirements with complete documentation of precise formulations of the set of processes covered by each. Create a detailed process, technical team, and schedule to allow for process-level modularization based on selected existing operational codes. Complete model level modularization of all supported codes and document code changes that must be incorporated into official code documentation. Develop standardized process level modules based on those common processes identified during model modularization. Examples could include a meteorology module (winds/pressures/precipitation), friction module, sediment module, and a standardized output file module to list a few examples. Modularized codes and associated code documentation will be setup in a version control repository for access across CHL and by external development partners, adhering to the numerical technology framework described above. Complete process level modularization of selected models by implementing the process modules developed in 4. An important element of component-based software is the ability to couple that software with other codes. In order to achieve this in an efficient way, the models need to be modularized into three basic parcels, an initialization phase, a run phase and a finalization phase. Equally import to this kind of modularization, is the need to move away from global data structures that are non-reentrant. Instead data structures need to be abstracted where internal representations of a data type are distinct from its external viewable/callable state. In short, this will allow multiple instances of the same model to be run within the same memory space without conflicts. The AdH suite has already made substantial progress toward this process. This report serves to complete the documentation required in the “Model Level Modularization Plan and Documentation” and the “Create Plan and Team for Process Level Components” plan mentioned in the numerated list above. The CHL makes use of many different numerical models and technologies, more than 30 and less than 100 different models/technologies, some of which include approved models but not recommended models by the HH&amp;C COP. In an effort to make the CBSP achievable, the following software/models have been identified as candidates for an initial modernization evaluation and are listed in no particular order. Adaptive Hydraulics (AdH) - AdH is a two and three dimensional unstructured finite element based suite of codes for solving baroclinic and barotropic flow conditions for shallow water (SW). It is routinely used for modeling sediment transport and is linked with water quality and species models for environmental studies. It supports a host of features vital to most hydraulic and transport-engineering applications, including spatial and temporal adaption, surface wave and wind-wave stress coupling, flow through hydraulic structures (locks, weirs, flap gate, etc.) and vessel flow interactions. Typical AdH applications include simulations of unsteady flows (tidal, hydrograph, etc.) and transport in rivers, estuaries and reservoirs. To remain on the forefront of ERDC-EQM development, AdH is also internally linked to process-oriented libraries for cohesive/non-cohesive sediment transport, meteorology, friction and turbulence applications. The ADvanced CIRCulation model (ADCIRC) – ADCIRC is best known as a two-dimensional, depth-integrated, barotropic time-dependent long wave, hydrodynamic circulation model. It also has a three dimensional shallow water formulation for both baroclinic and barotropic flows. ADCIRC models can be applied to computational domains encompassing the deep ocean, continental shelves, coastal seas, and small-scale estuarine systems. Typical ADCIRC applications include modeling tides and wind driven circulation, analysis of hurricane storm surge and flooding, dredging feasibility and material disposal studies, larval transport studies, near shore marine operations. ADCIRC is a highly developed computer program for solving the equations of motion for a moving fluid on a rotating earth. These equations have been formulated using the traditional hydrostatic pressure and Boussinesq approximations and have been discretized in space using the finite element (FE) method and in time using the finite difference (FD) method. CMS FLOW - The hydrodynamic circulation model CMS-Flow is a component of the Coastal Modeling System developed by the US Army Corps of Engineers Coastal and Hydraulics Laboratory. CMS-Flow is a two-dimensional, finite-difference numerical approximation of the depth-integrated continuity and momentum equations. Cells are defined on a staggered, rectilinear grid and can have constant or variable side lengths. The momentum equations are solved in a time-stepping manner first, followed by solution of the continuity equation, in which the updated velocities calculated by the momentum equations are applied. The model simulates currents, water level, sediment transport, and morphology in the coastal zone. CMS-Flow was designed to model navigation channel performance and sediment exchanges between the inlet and adjacent beaches in the coastal zone. Gridded Surface Subsurface Hydrologic Analysis (GSSHA) – GSSHA is a multidimensional modeling technology that uniformly couples overland, surface, and subsurface flow for accurate watershed simulation. It is a physics-based, distributed, hydrologic, sediment and constituent fate and transport model developed by the ERDC. GSSHA is routinely applied for both civil works and military applications. WaveWatch III – WaveWatch III is a third generation unstructured wave model developed at NOAA/NCEP in the spirit of the WAM model (WAMDIG 1988, Komen et al. 1994). It is a further development of the model WAVEWATCH, as developed at Delft University of Technology (Tolman 1989, 1991a) and WAVEWATCH II, developed at NASA, Goddard Space Flight Center (e.g., Tolman 1992). WAVEWATCH III®, however, differs from its predecessors in many important points such as the governing equations, the model structure, the numerical methods and the physical parameterizations. Furthermore, with model version 3.14, WAVEWATCH III® is evolving from a wave model into a wave modeling framework, which allows for easy development of additional physical and numerical approaches to wave modeling. STWAVE - STWAVE is a steady-state, finite difference, spectral model based on the wave action balance equation. STWAVE simulates depth-induced wave refraction and shoaling, current-induced refraction and shoaling, depth- and steepness-induced wave breaking, diffraction, wave growth because of wind input, and wave-wave interaction and white capping that redistribute and dissipate energy in a growing wave field. The purpose of STWAVE is to provide an easy-to-apply, flexible, and robust model for nearshore wind-wave growth and propagation. Recent upgrades to the model include wave-current interaction and steepness-induced wave breaking. STWAVE is written by the U.S. Army Corps of Engineers Waterways Experiment Station (USACE-WES). The method of analysis used by the STWAVE code along with the file formats and input parameters are described in the STWAVE documentation. 3.2 Software Evaluation Criteria The following criteria can be applied to somewhat quantitatively gauge the extent to which a given piece of software conforms to modern software practices, including the desired properties mentioned earlier. These criteria have been chosen largely based on past ERDC software development bottlenecks encountered in a significant number of projects. These bottlenecks have at times caused development delays, excessive funding additions and in some of the worse cases, an inability to meet final deliverables. Software lacking in one or more of these criteria may increase project risk and overall turn-around time. Linkable: This criteria asks if the given software is configured in such a way as to be linked with and executed by external software. For this to be possible, the software must have three callable phases of execution: Initialization – a callable routine that initializes the software. Oftentimes this means reading in suitable input files, memory allocation, etc. Run – a callable routine that executes the actual solution procedure of the software. For example, this routine might be responsible for integrating a partial differential equation. Finalization – a callable routine that finalizes the software. This could mean closing all open files, deallocating memory, etc. Note that ESMF (Earth, System Modeling Framework) compliancy is often used as a synonym for the “linkable” trait. It is not called that here because ESMF compliancy has evolved to require much more than the three phase software design given above. Distributed: This criteria asks if the given software can be executed on a High Performance Computing (HPC) machines in a distributed way. This is typically done by parallelizing the code via hard-wiring Message Passing Interface (MPI) calls into the software. MPI is a message passing library standard that is based on the consensus of the MPI Forum, which has over 40 participating organizations, including vendors, researchers, software library developers, and users. The goal of the Message Passing Interface is to establish a portable, efficient, and flexible standard for message passing that will be widely used for writing message passing programs. The advantages of developing message-passing software using MPI are portability, efficiency, and flexibility. MPI is not an IEEE or ISO standard, but has in fact, become the “industry standard” for writing message passing programs on HPC platforms. MPI libraries are also available for use on local multi-processor workstations (including Windows, Mac and Linux operating systems). Local: This criteria asks if the given software relies on local variables instead of global ones. Global variables are, in general, bad software practice for the following reasons: Non-locality - Source code is easiest to understand when the scope of its individual elements is limited. Global variables can be read or modified by any part of the program, making it difficult to remember or reason about every possible use. No Access Control or Constraint Checking - A global variable can be accessed (get) or set by any part of the program, and any rules regarding its use can be easily broken or forgotten. (In other words, get/set accessors are generally preferable over direct data access, and this is even more so for global data.) By extension, the lack of access control greatly hinders achieving software security in situations where you may wish to run untrusted code (such as working with 3rd party plugins). Implicit coupling - A program with many global variables often has tight couplings between some of those variables, and couplings between variables and functions. Grouping coupled items into cohesive units usually leads to better programs. Concurrency issues – If global variables can be accessed by multiple threads of execution, synchronization of that access is necessary (and too-often neglected). When dynamically linking modules with global variables, the composed system might not be thread-safe even if the two independent modules tested in dozens of different contexts were safe. Namespace pollution - Global variable names are available everywhere in a program. When that is the case, a programmer may unknowingly end up using a global variable but think they are using a local variable. This frequently can occur by misspelling or forgetting to declare the local variable or vice versa. Also when linking together different modules that have the same global variable names, if one is lucky, one will get linking errors. If one is unlucky, the linker will simply treat all uses of the same name as the same object. This same issue can also arise with file access. Memory allocation issues - Some environments have memory allocation schemes that make allocation of global variables tricky. This is especially true in languages where “constructors” have side-effects other than allocation (because, in that case, you can express unsafe situations where two global variables mutually depend on one another). Also, when dynamically linking modules, it can be unclear whether different libraries have their own instances of global variables or whether the global variables are shared. Testing and Confinement - Source that utilizes global variables is somewhat more difficult to test because one cannot readily set up a ‘clean’ environment between runs. More generally, source that utilizes global services of any sort (e.g. reading and writing files or databases) that aren’t explicitly provided to that source is difficult to test for the same reason. For communicating systems, the ability to test system invariants may require running more than one ‘copy’ of a system simultaneously, which is greatly hindered by any use of shared services - including global memory - that are not provided for sharing as part of the test. The next 4 criteria refer to possible process level modules/libraries that are being considered as part of the CBSD and which will be described in more detail later in this document. They are being mentioned now, in order to show how each library could be used in several of the numerical models for which CHL makes routine use. Those models are listed in Table 1, along with an assessment of the current status of each model in terms of the component based criteria listed above, as well as the applicability of these potential component based libraries listed next. In Table 1, an “N/A” is used if this library is not appropriate for this software. MetLib – This criterion asks if the given software is linkable to the Metlib static library for meteorological inputs. TransLib – This criterion asks if the given software is linkable to the Translib static library for sediment and general transport sourcing. TurbLib – This criterion asks if the given software is linkable to the Turlib static library for turbulence calculation. FrictionLib – This criterion asks if the given software is linkable to the Frictionlib static library for friction variable calculations. Table 3.1: Table B1. A list of current operational models and a description of their current status related to several component based modular criteria. An N/A is used if this library is not appropriate for this software. Note that (1) means that the software is compatible with WindLib which is a precursor to MetLib Linkable Distributed Local MetLib TransLib TurbLib FrictionLib ADH Y Y Y N(1) N N N ADCIRC Y Y N N(1) N N N CMS-FLOW N N Y N N N N Wave Watch III Y Y N N N/A N/A N STWAVE Y Y N N N/A N/A N GSSHA Y N N N N N/A N FUNWAVE N Y Y N N N/A N 3.3 External Library Linkage Work Plan The last four criteria involve linking software to external process libraries. These libraries are being proposed to collect and combine common development efforts required by a number of ERDC models. Such libraries provide a development focal point to eliminate funding redundancy among ERDC programs and push for common interfaces. They represent a key component of the CBSD. Tasks: This work plan addresses linkage of an already existing external library to a given piece of software. Although this process varies in complexity depending on the software and library, the following steps must usually be taken: Wrapper Construction: A wrapper file in the native language of the software must be created which links variables through memory to the library. Oftentimes, the wrapper must be capable of linking two different computing languages, involve coordinate transformations, and other spatial and temporal interpolation. (2 weeks 1 FTE) Build Modifications: The software build must be modularized to include the external library when appropriate. The AdH software, for example, utilized CMAKE to present a host of executable build options, easily checkable from a GUI interface. (1 week 1 FTE) *Note that “build” refers to the instructions used by computer language compilers to actually compile and create the software executable program. Verification: Once the wrapper is constructed and the software is capable of being built using the linked library, the linkage must be verified against analytic test cases and compared to previous unlinked versions of the software. (2 weeks 1 FTE) Although the development time required for software linkage varies depending on whether mixed languages, etc. are present, the complexity of the library/module, and the degree to which the model is already modularized, a conservative estimate for the above steps is four to five weeks for one full time employee (FTE) with knowledge of the software. The FTE should have expertise in computer science, particular in building executables with GNUMake and CMaKe. Models Potentially Benefiting:: AdH, AdCirc, CMSflow, WaveWatch III, StWave, GSSHA, Proteus, and FUNWAVE 3.4 Global Variable Removal Work Plan Global variables, as the name implies, are variables that are accessible globally, or everywhere throughout a program. Once declared, they remain in memory throughout the runtime of the program. This means that they can be changed by any function at any point and may affect the program as a whole. As mentioned, they are generally considered bad practice for the aforementioned reasons. For example, it is very easy for the programmer to lose track of values of global variables, especially in long programs, leading to bugs that can be very hard to locate. Source code is best understood when the scope of its individual elements are limited, so because of their non-locality, it is hard to keep track of where they have been changed or why they were changed. This is just one example of many as to why global variables should be avoided in modern software design. Tasks: The work plan for removing global variables from a given piece of software can vary rather dramatically depending on the current software framework. Additionally, since software variables are intimately tied to the framework of most codes, a robust testing phase is often required. Generally, the following tasks are needed to localize software variables: Assess the current state of the software to determine which global variables must be localized and create a software mapping plan for how these variables will be stored as instances and local variables. Implement the localization. Conduct extensive verification of the localized software for quality assurance Recent efforts for removing global variables from the AdH software have shown that it took one full time employee about six months of development and quality assurance to complete the process. The FTE must be a highly technical model developer, preferably proficient with both general computer science practices as well as in the physics of the software. This work can be a significant undertaking and there may be a limited number of people with the required expertise. Models Potentially Benefiting:: AdCirc, WaveWatch III, STWAVE, GSSHA, FUNWAVE 3.5 Software Distribution (HPC) Work Plan A distributed system consists of a collection of autonomous computers, connected through a network and includes distribution middleware, which enables computers to coordinate their activities and share the resources of the system. In this way a user perceives the distributed system as a single, integrated computing facility. Some advantages of distributed computing include: Highly efficient in terms of time to complete a simulation Scalability Less tolerant to failures High Availability As mentioned earlier the Message Passing Interface is a standardized and portable message-passing system developed for distributed and parallel computing. MPI provides parallel hardware vendors with a clearly defined base set of routines that can be efficiently implemented. As a result, hardware vendors can build upon this collection of standard low-level routines to create higher-level routines for the distributed-memory communication environment supplied with their parallel machines. MPI gives a user the flexibility of calling sets of routines from C, C++, Fortran, C#, Java or Python. The advantages of MPI over older message passing libraries are portability (because MPI has been implemented for almost every distributed memory architecture) and speed (because each implementation is in principle optimized for the hardware on which it runs). Tasks: Previous efforts to distribute legacy software using MPI for HPC applications have shown the level of effort involved to be an undertaking which largely depends on the legacy framework and structure of the code itself. More often than not, it can take several months to design a parallel paradigm for a code, implement that scheme, and tune it for better scaling performance. Additionally, since distribution (parallelizing) affects software variables that are intimately tied to the framework of most codes, a robust testing phase is required. The following 4 tasks are generally needed to complete this work plan: Assess the state of the legacy software to determine an optimal method for distribution. This should include profiling the code to locate wall clock bottlenecks using commercially available third party software. Distribute the software, i.e. use MPI code to parallelize. Verify the newly distributed software using the software’s standard test suite (if available) and comparing the serial and distributed results. Conduct a final software wall clock profile to ensure that all bottlenecks are addressed A conservative estimate for MPI distribution of serial software with adequate quality assurance would be 8 – 10 months for one FTE. Again, it is assumed that the FTE must have an extensive background in HPC architectures, the MPI language, and general computer science experience, plus knowledge of the physical/mathematical processes in the code is a major advantage also. Models Potentially Benefiting :: GSSHA, CMS-FLOW 3.6 Process Libraries This next section describes several possible process libraries that could be created for use by several of the CHL numerical models as part of the component based software design portfolio. 3.6.1 MetLib (Meteorological Library) Hydrodynamic modeling is a large industry in today’s engineering world, and as computational resources grow cheaper and faster, application domains grow in size and complexity. For these large scale hydrodynamic, hydraulic and transport problems, spatially varying meteorological input such as atmospheric pressure, wind velocities, precipitation rates, and temperature are required to drive dynamical simulations. These meteorological fields are most often computed a priori by external software and one-way coupled to the hydrodynamic software. More often than not, the meteorological data has a file format that differs from those used by the hydrodynamic software and may even be proprietary to the software used to create it or the instruments that collected and recorded the data. This typically means that a pre-processing step is required to change meteorological data from a given format to ones that are suitable for each hydrodynamic model. There is a great need within the ERDC modeling community for a generic meteorological input library with distributed (parallel) capabilities for use with hydrodynamic, hydrologic and transport models. This library could be linked to any hydrodynamic model and provide format interpreters and regridding capabilities, thereby eliminating the need for a separate preprocessing step. This would speed up the overall modeling process and reduce the possibility of human error. Furthermore, as the meteorological library is expanded, all the hydrodynamic models that use it immediately benefit. The Meteorological library proposed would provide atmospheric data by (1) reading in the meteorological data for a large variety of file format options and (2) efficiently interpolating that data both between time-snapshots and, when necessary, between the input meteorological and the hydrodynamic spatial grids, both of which can be structured or unstructured. The library would also contain parametric models such as the Holland vortex model (Holland, 1980) for tropical cyclones. Trahan et al 2017, unpublished technical report, have initiated development of such a library in their creation of the stand-alone static library WindLib, already routinely linked and used within the AdH model. WindLib leverages and was first created by extracting a subset of wind input capabilities from the ADvanced-CIRCulation shallow water model, ADCIRC. ADCIRC’s atmospheric input options are extensive, providing options for separate meteorological grids (subsequently interpolating onto the hydraulic mesh), time-interpolation between atmospheric snapshots, and numerous data input formats such as the US Navy Fleet Numeric format, Planetary Boundary Layer Hurricane Model format, NOAA’s Geophysical Fluid Dynamics Lab’s (GFDL) format, etc. Additionally, the ADCIRC model has several parametric wind models, such as the Holland vortex model, coded. To build WindLib from ADCIRC, pertinent routines were extracted and a parallel wrapper developed for use with AdH. For verification of WindLib portability, the library was linked to both AdH and ADCIRC, and the linkage verified against the current wind test suite for each model. WindLib can be used by different hydraulic suites, instead of manually adapting the data to a specific piece of software. Tasks: Development of MetLib will leverage the previous WindLib work and extend it to include the full suite of ADCIRC meteorological options and also those used by other hydrodynamic and transport models. The effort can be broken down into four major tasks: Extension of the WindLib static library to include new meteorological variables by either extraction from existing software or creation of new routines, all leveraging the existing work of WindLib. Creation of a Makefile that can build machine portable static libraries for linking to a variety of hydrodynamic models. Verification of the new meteorological variables from MetLib using software such as AdH. Note that these tasks are only for MetLib creation and verification and do not include efforts required to link a given hydrodynamic model to MetLib. AdH is mentioned in verification since it has already been linked to Windlib, the precursor to MetLib. In order to link MetLib to a general hydrodynamic model, a wrapper must be created specific to that model and the static library added to the executable build instructions and build process. Models Potentially Benefiting :: AdH, AdCirc, GSSHA, Proteus, CMS Flow, StWave, WaveWatch III, FUNWAVE (Future Use and R&amp;D) 3.6.2 TransLib (Transport Library) Pivotal to the ERDC environment quality mission is the need to simulate species/constituent transport in hydrodynamic systems. This is usually done by solving either (1) discrete particle trajectory Lagrangian equations or (2) continuous transport equations. Particle transport models can be used for a wide variety of projects, including modeling sediment, species and water quality dynamics. Although just about anything can be transported, particle-specific contributions can be condensed down to providing a mother transport equation with a few common items, advection corrections, sources/sinks, for example. There is a need for a common interface library that collects ERDC particle-specific routines for linking to both Lagrangian and continuous particle-based solvers. Importantly, TransLib, itself, will not solve the transport equation in any form. The library only provides a mother solver with species/constituent information. The following ERDC codes have particle processes that should be considered as inclusion into the TransLib: CORSED: A fundamental component of the TransLib library will be sediment. Sediment transport is pivotal to the ERDC environmental quality-modeling mission. Currently, there exist two separate sediment transport libraries, SedLib and SEDZLJ. Current R&amp;D efforts are in place to combine these libraries into a more general one named CORSED. The TransLib library will take this one step further and wrap CORSED into a general ERDC transport library. This library would then be capable of supplying to linked hydrodynamic models, corrections to sediment advection as well as supplying source and sink terms. PT123: PT123 is an ERDC particle tracking model designed to track massless particles in 1-, 2-, and 3-D unstructured or converted structured meshes. One adaptive (embedded 4th- and 5th-order) and three non-adaptive (1st-, 2nd-, and 4th-order) Runge-Kutta (RK) methods are included in PT123 to solve the ordinary differential equations describing the motion of particles. Both element-by-element (EBE) and non-element-by-element (NEBE) tracking approaches are incorporated into PT123. Recently, oyster physics was added into PT123. Oysters are a good representation of the ecosystem quality in estuaries, and are thus modeled quite a bit within the ERDC. The PT123 oyster processes will be extracted and added into TransLib. PTM: The Particle Tracking Model (PTM) is a Lagrangian particle tracker designed to allow the user to simulate particle transport processes. PTM has been developed for application to dredging and coastal projects, including dredged material dispersion and fate, sediment pathway and fate and constituent transport. The PTM has a host of species and water quality processes that can be extracted into the TransLib. Tasks: Development of TransLib will leverage the previous CORSED work and extend it to include species and water quality processes in PT123 and PTM. The effort can be broken down into the following tasks: Create a wrapper over the CORSED library to include it into TransLib (2 weeks) Extract PT123 and PTM water quality and species process routines and implement them into the TransLIb library (4 weeks) Create the TransLib static library (1 week) Verify Translib components (SedLib, SEDZLJ, PT123, PTM) (4 weeks) Create TransLib user documentation (1 week) Note that these tasks are only for TransLib creation and verification and do not include efforts required to link a given hydrodynamic model to the library. In order to link TransLib to a general hydrodynamic model, a wrapper must be created specific to that model and the static library added to the executable build (see previous work plan). This work requires 1 FTE for development, but the coordination of many ERDC software teams to implement. The developer should have expertise in computer science, software design, particle tracking methods and hydrodynamics. Models Potentially Benefiting :: PT123, PTM, AdH, ADCIRC, CMS-Flow 3.6.3 TurbLib (Turbulence Library) When Reynolds averaged models are utilized, a turbulence model is needed to estimate the Reynolds stresses in the governing equation. Most often, these stresses are defined in terms of known averaged quantities. There are a number of ways to do this, for example, by using transport or non-linear eddy viscosity models. Turbulence closure models/equations are used to account for processes that arise as a result of chaotic changes and/or gradients in pressure and velocity. These changes increase the hydraulic/hydrodynamic losses in the flow field, most often through the generation of vortices that may or may not interact with each other. Currently, the AdH software suite has a hard-wired turbulence library that supports a host of turbulence models and options. These files can be leveraged to create a stand-alone library for linking to other hydrodynamic models. Within the AdH model, turbulence routines are written in a generic length scale format. This convenient set-up only requires that the linked model send coefficients that match the different turbulence options. Current AdH turbulence options include: Horizontal Eddy Viscosity and Diffusivity (sub-grid scale models) Smagorinsky Formulation Vertical Eddy Viscosity and Diffusivity (Boussinesq hypothesis models) Mellor-Yamada Level 2 Mellor-Yamada Level 2.5 Kappa-epsilon Buoyancy Supression Options Henderson-Sellers Munk-Anderson Kent-Pritchard Pritchard French-McCutcheon Another option being consider by the team is whether instead of creating TurbLib from scratch, could the existing community developed General Ocean Turbulence Model (GOTM) be used instead, or could AdH’s routines be used along with the GOTM routine to provide a much wider range of options. GOTM has been being developed for over 20 years and has an extensive user base. For more details, see the GOTM website (https://gotm.net/). Tasks: This work plan can be broken down into three stages: Extraction of the current AdH turbulence library so that it is buildable as a stand-alone static library for linking to other software. (2 weeks) Testing the new turbulence library by linking to AdH and comparing results to previously hard-wired turbulence options. (5 weeks) Testing linkage of GOTM to AdH. A comparison of results from ADH compared to its original turbulence options (where appropriate) as well as a comparisons of computation performance using GOTM. (5 weeks) Final recommendation on using AdH’s turbulence options, the GOTM library, or a combination of the two for TurbLib to be used by other hydrodynamic models. (1 week) User manual creation. (1 week) This effort requires one FTE skilled in turbulence theory, software development and basic fluid dynamics. Models Potentially Benefiting :: AdH, AdCirc, CMSflow, Proteus 3.6.4 FricLib (Friction Library) Generally, hydrodynamic models resort to the application of some sort of drag law near the -bed to represent the turbulent frictional processes there. The use of lateral (i.e. horizontal) friction, on the other hand, is an almost universal constraint imposed on all hydrodynamic numerical models by their inability to follow the nonlinear cascade of energy below scales of the mesh size. This friction effectively prevents the pileup of energy in a numerical simulation in the wavelength range corresponding to the mesh size. Hydrodynamic models, in general, apply frictional effects as a stress in the momentum equations. Empiricism is imparted to this frictional stress term through the usage of a “coefficient of drag” or “coefficient of friction”. This coefficient is dependent upon characteristics of the bed boundary layer. The library proposed here will supply a variety of drag coefficients to a linked hydrodynamic/wave model. The library can be built by leveraging current friction routines within the Adaptive Hydraulics suite. AdH currently has the ability to calculate a number of friction coefficients for hydrodynamics equations using physically relevant formulations to accurately account for friction losses by calling a friction library of routines for coefficient calculation and/or conversion. A few of the most widely used friction library calculations will now be described. Bed Shear Coefficient of Friction - computes a shear stress coefficient resulting from a steady (or quasi-steady) current field. The formulation given here is derived from a modified form of the classic logarithmic velocity profile. The traditional profile yields a velocity of infinity at the bed; whereas the modified profile forces the velocity to vanish at the bed. Submerged Aquatic Vegetation Coefficient of Friction - computes a shear stress coefficient resulting from a steady (or quasi-steady) current field over a bed consisting of submerged aquatic vegetation (SAV). Unsubmerged Rigid Vegetation Coefficient of Friction - computes a shear stress coefficient resulting from a steady (or quasi-steady) current through rigid, unsubmerged vegetation. Some examples of this might include flow through mangrove stands, phragmites in coastal wetlands, or trees and other obstructions in coastal storm surge flooding. The formulation includes both the form drag induced by flow through the obstructions and the skin drag induced by flow over the bed. Evenly Distributed Obstruction Coefficient of Friction - computes a shear stress coefficient for use in computing the shear stress resulting from a steady (or quasi-steady) current through or over an evenly distributed field of flow obstructions. This formulation can be used to simulate flow through or over wetland vegetation, trees, buildings or any other subgrid-scale obstructions. The obstructions are modeled as a field of evenly distributed cylinders. Tasks: This work plan can be broken down into three stages: Extraction of the current AdH friction library so that it is buildable as a stand-alone static library for linking to other software. (2 weeks) Testing the new friction library by linking to AdH and comparing results to previously hard-wired turbulence options. (5 weeks) User manual creation. (1 week) This effort requires one FTE skilled in finite element methods, software development and basic fluid dynamics. Models Potentially Benefiting :: AdH, AdCirc, CMSflow, Proteus, FUNWAVE, STWAVE, and WaveWatch III "],["appendix-tbd.html", "Chapter 4 Appendix TBD", " Chapter 4 Appendix TBD Version: 1.0.0 Date: 2019.03.18 Written by: authors (email) Add other text later. "],["appendix-tbd-1.html", "Chapter 5 Appendix TBD", " Chapter 5 Appendix TBD Version: 1.0.0 Date: 2019.03.18 Written by: authors (email) Add other text later. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
