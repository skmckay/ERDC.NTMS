[["index.html", "Community based Numerical Technology Modernization Strategy Overview Mission Goals Glossary and Acronymns Points of Contact", " Community based Numerical Technology Modernization Strategy ERDC Coastal &amp; Hydraulics Laboratory 2019 Overview RECREATION OF 2019 DOCUMENT AS EXAMPLE. Could include example of version control in documentation format. Version.Patch.Bug becomes Publication.SectionRevision.Typo? Mission The mission of the Coastal and Hydraulics Laboratory (CHL) is to deliver solutions to our Nation’s most challenging coastal and hydraulic problems through research, development and application of cutting-edge science, engineering and technology. To support this mission, CHL is adopting this Numerical Technology Modernization Strategy (NTMS) to revolutionize the way it develops, applies, transitions into practice, and maintains numerical technologies developed for Civil Works and Military applications. The goals of the strategy are to guide CHL’s investments in numerical and data technologies; to improve numerical modeling and data integration infrastructure; to establish a common system for digital technology development and maintenance, considering the entire business life cycle (from development to retirement) of CHL’s software and data products; to improve and integrate relevant data and numerical technologies, from simple tools and calculators to expert modeling systems, so that they are efficient to access, apply, and cost less to use and maintain; and to document a plan to facilitate interagency collaboration as well as for leveraging and growing resources. Goals Guide investments in numerical and data technologies. Improve numerical modeling and data integration infrastructure. Establish a common system for digital technology development and maintenance. Improve and integrate relevant data and numerical technologies. Make data and numerical technologies efficient to access and apply while costing less to use and maintain. Document the vision to foster ERDC, interagency, and academic collaboration and to leverage resources Glossary and Acronymns ADCIRC: “a system of computer programs for solving time dependent, free surface circulation and transport problems in two and three dimensions. These programs utilize the finite element method in space allowing the use of highly flexible, unstructured grids.” For this study, ADCIRC was used to predict storm surge. AdH: Adaptive Hydraulics (AdH) numerical code is “a modular, parallel, adaptive finite-element model for one-, two- and three-dimensional (2D, and 3D) flow and transport.” For this study, AdH was used to predict changes in local hydrodynamics. CHL: ERDC’s Coastal &amp; Hydraulics Laboratory. NTMS: Numerical Technology Modernization Strategy. ERDC: U.S. Army Engineer Research and Development Center. USACE: U.S. Army Corps of Engineers. Points of Contact Chris Massey, Ph.D., Coastal &amp; Hydraulics Laboratory, U.S. Army Engineer Research and Development Center, Vicksburg, Mississippi Email: Chris.Massey@erdc.dren.mil Tahirih Lackey, Ph.D., Coastal &amp; Hydraulics Laboratory, U.S. Army Engineer Research and Development Center, Vicksburg, Mississippi Email: Tahirih.C.Lackey@erdc.dren.mil "],["numerical-technology-modernization-strategy.html", "Chapter 1 Numerical Technology Modernization Strategy 1.1 Introduction 1.2 Reinvented solution workspace 1.3 Unified data and metadata standards 1.4 Verification, validation and uncertainty quantification 1.5 Component based software 1.6 Community partners 1.7 References", " Chapter 1 Numerical Technology Modernization Strategy Version: 1.0.0 Date: 2019 Written by: Chris Massey (Chris.Massey@usace.army.mil) 1.1 Introduction This is a living document, intended for continuous review and updating, at least annually, as progress is made on the implementation of the NTMS and as new technologies emerge. While the NTMS started within CHL, success in implementing the strategy will only be realized with the participation of other ERDC labs, USACE districts, as well as partner federal agencies. The numerical technology modernization strategy strengthens the entire coastal and hydraulics enterprise by equipping practitioners with the best numerical tools available. In so doing, it will perpetuate CHL as a continued leader in pioneering coastal and hydraulic numerical technology research and development and will integrate expertise and resources across business, program, and organizational lines. Numerical models are increasingly seen as a commodity, particularly in an ever-increasing open source development culture, and this strategy will guide CHL within this culture, to relentlessly pursue collaborations in development of leading edge technologies. CHL will work alongside our partners to transition state-of-the-art to state-of-the-practice through enhanced customer support and technology transfer efforts. In this document, the word “technologies” is used to encompass models, tools, data and information technology (IT) infrastructure. ”Models” refers to numerical computer algorithms, e.g. computer software that solves systems of equations. “Tools” refers to mechanisms that access, create, and/or manipulate data, such as graphical users interfaces (GUIs) and cloud/web-based calculators. “Information Technology (IT) infrastructure” refers to computing hardware systems, data storage systems and internet access/bandwidth. Support software and project management software, such as “git” for version control and “Jira” for project management software, overlaps both Tools and IT infrastructure. The three pillars of the NTMS are component-based community software; seamlessly integrated data and models; and cloud/web-based infrastructure/tools. These components are reinforced and interconnected with an integrated verification, validation and uncertainty quantification (VVUQ) process. The VVUQ process continually ensures the integrity of the components and indicates directions for needed research and development improvements. The strategy seeks to layout the necessary foundations for integrating the three pillars to achieve a reinvented solution workspace that has both process driven computational capabilities and data that are seamlessly interchanged and are cloud environment enabled. The workspace refers to more than the tools, e.g. interfaces, and data, it also encompasses the workflow processes as well as the access to and resources upon which everything is executed and stored, e.g. infrastructure. The seamless interchange between models, tools, and data requires that all are readily accessible across operating platform environments, which is particularly relevant in light of big data analytics. The user experience will evolve into a model agnostic interface that is customizable, yet familiar and is integrated with best practices and USACE guidance documents along with training and user manuals. In order to be accepted and trusted by the end users, these technologies must first be accurate and robust, have appropriate user support and training, and be well validated and verified for the problems they are solving. The NTMS can be broken down into five major action areas: Teams have been formed for each of the first four major areas in order to further flesh out details and needs, and then make recommendations and resource estimates for implementing the strategy in a series of plans. Community partners will be engaged to expand the strategy and to help formulate plans for implementing it. The five action areas are described in the remaining sections of this document. 1.2 Reinvented solution workspace The purpose of having a reinvented solution workspace is to transform the current models, tools, e.g. interfaces, and workflows into a common environment that operates on anticipated future computational resources, including cloud environments. Furthermore, it will have a common look and feel to modeling and data processes across the spectrum of modeling, which reduces learning curves. At the same time, the new workspace allows for customizable workflows that are in line with USACE guidance and community best practices. The new workspace is required because how we as a community of practice interact with models and data in the future will be very different than today. Big data, the computational complexity of modeling systems of systems, and the proliferation of portable handheld devices are driving forces moving the community toward cloud-based computing and web/app-based applications. Our tools must be collocated with future computation resources and allow ready access to required data, both inputs and outputs. To create this new paradigm, new tools for model/data interaction are required. These requirements are in part responsible for the need to implement the other components of the NTMS. For example, using unified data and metadata standards for stored data and for use in both complex numerical models and simpler on-line tools, helps to facilitate a model agnostic interface and reduce workflow complexities by removing reformatting. This model agnostic interface is a significant part of the design concept for the new workflow. Table 1.1: Dimensions of a reinvented solution workspace. 1. Common environment (cloud) 2. Seamless model-data interaction 3. Model agnostic interface 4. Customizable workflows 5. Integrated with guidance documents Each of the individual components will have a similar look and feel with a common branding, a unified naming convention that is not tied to an organization. A new common branding for the workspace solutions does not mean that existing names of models/tools will change. To illustrate suppose that WIDGIT is the common branding name, then WIDGIT-Hydro would be the name of the process-based workflow that solves the shallow water equations. That workflow would use existing models like ADCIRC, AdH, and CMS-Flow as the solution engines. Each of those models have varying degrees of fidelity, speed of computation, uncertainty, and complexity of setup and execution. The new processed based workflow WIDGIT-Hydro and the new interface would eliminate the user from having to deal with individual model specific requirements. The new interface must also be configurable and allow individual processes to be linkable by the users in a customizable way. The workspace must have readily available preconfigured coupled workflows that exist as selectable options. These workflows will be informed by and comply with USACE guidance and best practice procedures. With these preexisting workflows, the reinvented solution workspace can have a question and answer process that leads a user through a series of guided questions to determine not only the correct process to use, but also the appropriate “modules” to apply based on specified error tolerances and desired level of effort. Expert users would still have the ability to select specific models for use instead of being guided. For this new workspace to be realized, numerical models and tools will be platform independent in their execution. Specifically, large operational numerical models need flexibility to be compiled and run under native operating systems, such as Windows, Mac OS, and Unix/Linux. The models will have a common file type and data structures to aid in data exchanges. They will be designed or re-designed to be scriptable and callable so that they can be incorporated into larger workflows by coupling and integration with other applications and be executable under or through web-based applications. Each model and tool will be analyzed consistently, fairly, and expertly for accuracy and applicability to target problems to aid the model/data selection process. Furthermore, metrics are required to evaluate the complexity and skill sets needed for setting up and executing the appropriate workflows to solve engineering problems. Having such metrics and measures will facilitate filtering of workflow components for different applications and budgets. These are just a few highlights of the traits and capabilities of the reinvented solution workspace of the future and some of the changes needed to achieve it. A whitepaper is currently being developed, led by a team of experts from CHL with input from users both within USACE districts and from external partners, in order to develop plans for implementation, particularly the workspace interfaces. The whitepaper will also document what has worked well with existing user interfaces, their development process and their user tech support procedures. In this way, best practices can be maintained and carried forward. It is anticipated that external partners will help to advance these new technologies through open-source licenses and peer-reviewed and refereed processes. 1.3 Unified data and metadata standards The purpose of the unified data/metadata standards is first to make files self-describing in content and structure to ensure details about the file contents are known, allowing users to make informed decisions about the suitable use of the data. Secondly, when fully implemented, the standards will facilitate certain types of numerical model coupling, make them easier to use and more transportable. A unified data structure means a single file-type/format and a commonly adopted single method for describing and writing the data in the file. Using a unified data structure will allow each model to be capable of reading and writing a file that is immediately recognizable and usable to other models of similar types. It also means that interfaces and downstream products and applications, like the Coastal Hazards System (CHS), do not require the data to be reformatted prior to use and can have access to many more data sources than are currently available. The use of the Network Common Data Form (NetCDF) file types has been adopted as a common starting point along with the adoption of the Climate-Forecast (CF) Standards for metadata and file structures (CF-Conventions, 2020). The NetCDF4 (Unidata, 2020) library is readily available across multiple computing platforms, which allows for easier porting and exchange of files. It is designed to read and write data in a well-defined and structured way. Various other federal agencies including the National Oceanic and Atmospheric Administration (NOAA) and the United States Geological Survey (USGS) have adopted NetCDF file types. Metadata details all aspects of a data type and a file in such a way that a user or a software suite can understand the contents of the file and have information about when, where, and how the data were produced. Standard naming conventions aid in removing uncertainty in the meaning of a data variable and allows for easier discovery of data within a given file by automated systems. The unified data integration team has recommended that metadata inclusion be broken down into three classifications: Table 1.2: Classifications of metadata inclusion. 1. Required values 2. Referred values 3. Optional values This tiered strategy is aimed at making sure the most important metadata gets included in every file so that data are not rendered useless if it is missing. For example, having calculated solution values from a numerical model, but not knowing their coordinate system, units of measure and quality control ratings can render the data useless, while not knowing the date the model was executed is less critical. The unified data integration team is working on a series of whitepapers to specify a standardized format for data types, e.g., waves, circulation hydrodynamics, sediments, water quality and species, in collaboration with the Environmental Lab, to document standards for metadata inclusion. The documentation for waves is the first set of guidelines to be developed and is currently being refined (August/September 2020). Circulation hydrodynamics is next to be completed in (September/October 2020), and the other process guideline whitepapers will be completed by December of 2020. 1.4 Verification, validation and uncertainty quantification There is an old saying attributed to George Box (1978) “Models, of course, are never true, but fortunately it is only necessary that they be useful.” The purpose of VVUQ is to document model accuracy and uncertainty. Ensuring models are solving what they claim they are solving is paramount in building trust in their application. Verification determines if the computational model fits the mathematical description of the process while validation is performed to determine if the model accurately represents the real world application. Robert O’Keefe and Daniel O’Leary (1993) give a simple yet very useful definition of the two as “Verification refers to building the system right, while validation refers to building the right system.” Uncertainty quantification attempts to quantitatively identify, classify, estimate, and when possible reduce uncertainties in both numerical models and measured/collected data. Philip Stark (Stark, 2017) describes it this way, “UQ tries to appraise and quantify the uncertainty of models of physical systems, which are calibrated to noisy data (and of predictions from those models), including contributions from incomplete knowledge of a process; systematic and stochastic measurement error; limitations of theoretical models; limitations of numerical representations of those models; limitations of the accuracy and reliability of computations, approximations, and algorithms; and human error.“ Since models are continually being developed, improved and otherwise changed, and the same for data collection methods, it is important to have procedures in place that can assess the impacts of changes. For numerical models, it is important to be sure that software changes have not broken the model so that it no longer executes, or worse, executes and produces wrong or unintended results. When new/different instruments are used to collect measured data, it is important to know how the error tolerances (precision of the instrument) in the new systems differ from the old system. As part of the NTMS, automated software regression tools will be implemented along with version control software and repositories to keep track of code changes and generate alerts to changes in software performance. The Verification, Validation and Uncertainty Quantification (VVUQ) team sent out a survey to team leads for all major CHL developed and used numerical models. The survey captured existing VVUQ processes and cases for each of the models. The results of that survey were summarized into a whitepaper in March 2019, and the survey results serve as a jumping off point for developing a detailed and comprehensive set of procedures and metrics for implementing standardized and consistent VVUQ process for all models. These test cases are physics-based and not just model-based. They will consider the full range of VVUQ from analytic to lab and field tests including the Model Testbed work (Bak, 2016) at the Field Research Facility (FRF). Some of the VVUQ test cases can also double as training materials for the process models and will facilitate the automated regression testing capabilities. These automated regression test are typically performed on regular schedules, sometimes nightly, and will be required to be performed and documented as part of any new release versions of codes, both complex models and simpler one-line tools. Furthermore, software developers would apply these tests frequently while making code changes or implementing existing codes on new platforms. 1.5 Component based software The purpose of component-based software is to design and organize numerical codes into process based functions and routines, in order to streamline changes and updates to large numerical codes, as well as to share and take advantage of common process libraries. Whether large or small, having numerical codes organized into single processes or functions makes them easier to debug and update. Furthermore, models should be broken down into three main tasking phases: initialization, run, and finalization. Table 1.3: Dimensions of component based software. 1. Modular code - process based design 2. Use of community software libraries 3. Coupling framework ready Initialization is the startup process in which variables are typically setup and sized properly and other onetime execution events happen. The run phase is the main component or purpose of the model and is one that might get repeated many times in an overall simulation. The finalization phase is where the model resources are deallocated and the model shuts down in a controlled process. The phased approach also aids in the models being called by external models, which is very useful in model coupling and other scriptable procedures. Having models further broken down into process level components can allow for reuse of code and build interchangeable process libraries. For example, making use of the general ocean turbulence model (GOTM) library in three-dimensional circulation models, or matrix solvers packages like the Portable, Extensible Toolkit for Scientific Computation (PETSc), can increase model solution accuracies while decreasing model development times. In the same fashion, development of libraries, or process-based components, allows updates to be implemented quickly into models using them. The component-based software team developed a whitepaper in March 2019 that outlines possible process level modules (sediments, meteorology, turbulence, friction, etc.) that are common to several numerical technologies and that could be developed into standalone libraries that are then called by individual models. The whitepaper also details other important modern software practices such as the ability of models to be linked with other models via a coupling framework like the Earth System Modeling Framework (ESMF) and the Coastal Storm Modeling Systems Coupling Framework. Some numerical models have already implemented portions, but not all, of these practices while others will require substantial code rewrites to become fully compliant with such practices. The whitepaper also details some of those specific requirements. The team will work with individual model leads/developers to outline a comprehensive work plan and detailed resource needs for each model to become fully compliant. 1.6 Community partners While CHL is initiating the first version of the NTMS and beginning to develop plans for implementing portions of the strategy, it is highly desirable that the strategy become a community strategy. Other ERDC labs, USACE districts and centers of expertise are especially important partners. Sister federal organizations such as the National Oceanic and Atmospheric Administration (NOAA), the U.S. Geological Survey (USGS) and the U.S. Bureau of Reclamation (USBR) have overlapping modeling needs with those of CHL and in some cases use the same numerical models. Even if individual models are shared, the workflows and data standards are not the same, nor are the tools accessing them. Development is also not traditionally coordinated. Using open source development, unified data standards, community accepted test cases and following a common strategy to guide current and future developments, not only makes good sense, it makes good use of limited resources. Our federal partners are invited to join in refining the NTMS and in helping to formulate plans for implementation. This is somewhat happening now with the partnership at the National Water Center through the Coastal Coupling Community of Practice and recent meetings about the next generation National Water Model, as well as co-development on both ADCIRC and WaveWatch III models. Other partners could be from the Department of Defense (DoD) to include the Army, Navy and Air Force as well as the DoD’s High Performance Computing (HPC) Modernization Program’s CREATE, which stands for Computational Research and Engineering Acquisition Tools and Environments. The CREATE program is designed to improve the DoD’s acquisition process by developing and deploying advanced computational engineering design tools for acquisition programs within military aircraft design; military ship design; RF antenna designs and their integration with platforms; and military ground vehicle designs. The program has flexible operating platforms on the DoD HPC environment as well as a web-based portal. Currently, CHL and the CREATE program leads are exploring ways to work together. Academic partnerships are also desirable for many reasons. With academia’s help, models and tools can be co-developed allowing government and industry users’ access to updated technologies faster. At the same time, the community based open source model codes, training materials and example cases from government and private sector, could serve as curriculum materials and be taught in university classes. This would mean that new graduates are already versed in the models that they will use on the job. Corporate partnerships are also desirable. Aquaveo Inc. is the main developer of the USACE enterprise level GUI’s for hydrodynamic models that are in use today. They are willing to partner to adapt their technologies to meet the future need of operating on the cloud and use of adaptable/configurable workflows. Their expertise in geospatial graphical interfaces and experience applying the same tools is invaluable. 1.7 References Bak, S., (1996). Computational Model Test Bed (CMTB): Numerical Model Testing and Evaluation at the FRF. Accessed on 8/20/2020, from https://www.erdc.usace.army.mil/Media/Fact-Sheets/Fact-Sheet-Article-View/Article/756547/coastal-model-test-bed-cmtb/. Eaton, Brian, Jonathan Gregory, Bob Drach, Karl Taylor, Steve Hankin, Jon Blower, John Caron, Rich Signell, Phil Bentley, Greg Rappa, Heinke Höck, Alison Pamment, Martin Juckes, Martin Raspaud, Randy Horne, Timothy Whiteaker, David Blodgett, Charlie Zender, Daniel Lee, 2020. “NetCF Climate and Forecast (CF) Metadata Conventions”, v1.8. Accessed on 6/16/2020, from http://cfconventions.org/Data/cf-conventions/cf-conventions-1.8/cf-conventions.pdf. GOTM, “General Ocean Turbulence Model”, accessed online from https://gotm.net/. O’Keefe, R.M., O’Leary, D.E., (1993). Expert system verification and validation: a survey and tutorial. Artif. Intell. Rev. 7, 3–42. PETSc, “Portable, Extensible Toolkit for Scientific Computation”, accessed online from https://www.mcs.anl.gov/petsc/. Stark, Philip, 2017. “Uncertainity Quantification, Lecture 1.” Accessed online on June 1, 2020 from https://www.stat.berkeley.edu/~stark/Seminars/lesDiablerets17-1.pdf. Unidata, 2020. NetCDF4.7.4. Boulder, CO: UCAR/Unidata Program Center. Accessed online on June 13, 2020 from https://www.unidata.ucar.edu/blogs/news/entry/netcdf-4-7-4. "],["verification-validation-and-uncertainty-quantification-teams-whitepaper-for-the-numerical-technology-modernization-plan.html", "Chapter 2 Verification, Validation and Uncertainty Quantification Team’s Whitepaper for the Numerical Technology Modernization Plan 2.1 Introduction 2.2 Approach 2.3 Results 2.4 Recommendations 2.5 References", " Chapter 2 Verification, Validation and Uncertainty Quantification Team’s Whitepaper for the Numerical Technology Modernization Plan Version: 1.0.0 Date: 2019.03.18 Written by: Matthew Farthing and Chris Massey (Chris.Massey@usace.army.mil) 2.1 Introduction 2.1.1 Purpose There is a rich literature on Verification Validation and Uncertainty Quantification (VVUQ) across a range of disciplines from nuclear engineering to aeronautics and operations research. The purpose of this whitepaper is not to cover these topics in detail. For excellent reviews and textbooks we recommend (among others) [Oberkampf and Trucano, 2002, 2008, Oberkampf and Roy, 2010] as well as the documentation and guidance from groups such as the Defense Modeling and Simulation Coordination Office (DMSCO), https://vva.msco.mil, and more recent efforts in reproducible scientific research [Piccolo and Frampton, 2016; Nature 2018 ]. Rather, our goal is to provide a summary of on-going VVUQ practice in a key set of models and to recommend steps for establishing a lab-wide VVUQ process as part of the Coastal and Hydraulics Laboratory (CHL) Numerical Technology Modernization Strategy (NTMS). 2.1.2 Motivation and Background Computational modeling and simulation are essential tools in the U.S. Army Corps of Engineers (USACE) efforts to address the Nation’s water resources challenges. To do this effectively the models developed, and applied, must be credible. That is, users must have confidence that the simulations from a computational model have acceptable levels of uncertainty and error [Oberkampf and Trucano, 2002]. There are many ways to define error and uncertainty. As working terms, we will adopt those provided by the American Institute of Aeronautics and Astronautics (AIAA) in their 1998 “Guide for the verification and validation of computational fluid dynamics simulations” [AIAA, 1998]. Error is a recognizable deficiency in any phase or activity of modeling and simulation that is not due to lack of knowledge. Uncertainty is a potential deficiency in any phase or activity of the modeling process that is due to the lack of knowledge. Roache (1997) refers to quantification of uncertainty as estimation or banding of the numerical error in a numerical solution as it is used for decision making. More recently, one often sees uncertainty divided into two categories: epistemic and aleatory uncertainty. Epistemic uncertainty is considered to be due to insufficient knowledge, and as such, can be controlled or reduced through additional investigation and improved representation/resolution of a particular system. Aleatory uncertainty on the other hand is due to noise or stochasticity inherent in a system. As such, it is considered to be irreducible [Asch et al, 2016]. Here, we will use the term uncertainty to cover both categories unless noted explicitly. The primary tools for assessing the accuracy of a computational model and the simulations it produces are known as Verification and Validation (V&amp;V). Verification is the process of determining that a model implementation accurately represents the developer’s conceptual description of the model and the solution to the model [Oberkampf and Trucano, 2008]. Validation is the process of determining the degree to which a model is an accurate representation of the real world from the perspective of the intended uses of the model [Oberkampf and Trucano, 2008]. Loosely, verification determines whether or not the equations underlying a computational model are being solved correctly, while validation addresses the question of whether or not the correct equations are being solved. We also distinguish validation from calibration. By calibration we mean the process of adjusting numerical or physical modeling parameters in the computational model for the purpose of improving agreement with experimental data [AIAA, 1998]. Finally, the process of accreditation is the official certification that a model or simulation, or federation of models and simulations, and its associated data is acceptable for use for a specific purpose [DMSCO, 2011]. It is also worth noting at this point that in the context of the DoD Modeling and Simulation (M&amp;S) community, the real-world may include composite systems with natural, automated, and human components [DMSCO, 2011]. Here, we will primarily consider validation for the physical subsystems. Verification includes code verification and solution verification. Solution verification determines the numerical accuracy of computational results. It typically involves quantifying the level of discretization error due to approximating temporal and spatial dynamics as well as the error due to resolving linear and nonlinear systems of equations [Oberkampf and Trucano, 2002]. Code verification encompasses determining whether or not the computational model correctly implements numerical algorithms as well as the entire solution process from reading input to archiving solutions, output quantities of interest (QoI), and run statistics. In other words, code verification requires finding and eliminating programming errors (bugs) through proper testing and software development practice. An associated type of error is usage error, which becomes more and more likely with the complexity of a model. Usage error can be controlled through proper documentation and training. Solution verification leans more heavily on techniques from applied mathematics, while code verification requires techniques from software engineering. Both are within the domain of computational science and engineering [Heroux and Willenbring, 2009]. Validation is more challenging than verification, since it requires assessing whether or not a model’s analysis reflects the real world adequately. In fact, determining the level of error resulting from the numerical discretization is a prerequisite. Fundamentally, validation involves comparing computational model results with experimental data. Quantifying the level of error in the experimental data is then also required before the agreement between a model’s conceptual formulation and the physical system can be assessed clearly. Verification and validation are processes in which certain qualities of a model and its simulations are assessed. Given their complexity, computational models cannot be verified theoretically nor proven to be valid. Rather, they can only be shown to be incorrect or disproved [Oberkampf and Trucano, 2002]. The cornerstones of the VVUQ process are then benchmarks and metrics. That is, the VVUQ process is only as good as the problems through which we evaluate the model and the quantitative measures we use to determine model performance. Verification benchmarks are usually analytical solutions or trusted, highly accurate numerical solutions. Analytical solutions are obviously preferable, but they are typically available only in cases of highly simplified geometries, auxiliary conditions, and/or material parameters for the classes of models of interest to the lab. Techniques like the method of manufactured solutions [Katz and Sankaran, 2011] can help extend the range of available solutions, but reference numerical solutions and comparison with other codes are still important sources of benchmarks. Metrics for verification tests include global norms of the solution error, conservation of mass and energy, and/or maintenance of positivity and/or symmetry if appropriate. It is also important to evaluate the performance of the computational model versus expected behavior of the numerical methods and algorithms it applies. For example, establishing that a solution converges at the expected rate as the spatial and/or temporal discretization increases can be a valuable way to build confidence that a code is behaving as intended [Roache, 1997]. As mentioned above, code verification requires tools and techniques from software engineering. The mapping of formal techniques from software engineering to simulation codes for hydrodynamics and other environmental processes is not always straightforward given the fact that we are often expanding the formulations and/or approximations in our models to better understand and solve engineering problems [Heroux and Willenbring, 2009]. However, minimal requirements like source control and documentation, cataloging of errors (bugs), and automated regression testing have become much easier and accessible over the last decade thanks to tools like gitlab, github, and bitbucket, which integrate source control with support for these development components (and more). Validation benchmarks involve comparison of computational results with data collected from experiments. These experiments include not just observations of the system but also estimated values for physical parameters, initial conditions, and boundary conditions needed for the model simulation. Again, estimates for the uncertainty in these values is also essential. The problems of interest to CHL and ERDC often involve a number of complex processes interacting at multiple scales. Validation of these multiphysics/multiscale problems is particularly difficult, and traditional validation experiments may only be available at the lab or “bench” scale. These experiments often involve not just a reduction in scale but also in the processes that are modeled in order to diminish the number of parameters that are unknown or poorly known. This is akin to the process of decomposing a problem into a hierarchy of phases or subsystems that is recommended by among others [AIAA, 1998]. Validation experiments at larger system scale are certainly possible and are a strength of ERDC in some areas (e.g., through the Field Research Facility at Duck). We will refer to these as field-scale validation experiments. In our view, the essence of a validation experiment is that it is designed with the intention of validating computational models in mind, the necessary physical parameters and auxiliary conditions are known and the primary sources of uncertainty are accounted for and quantified [Oberkampf and Trucano, 2008]. This is obviously a fluid definition that will depend on the physical system, the model, and its intended uses. Generally, one can define Strong Sense Benchmarks (SSBs) as test problems that have the following four characteristics [Oberkampf and Trucano, 2002]: the purpose of the benchmark is clearly understood, the definition and description of the benchmark is precisely stated, specific requirements are stated for how comparisons are to be made with the results of the benchmark, and acceptance criteria for comparison with the benchmark are defined. The notion of an SSB can apply both to verification and validation test problems. Detailed recommendations for SSBs can be found in [Oberkampf, 2008]. Finally, it is important that benchmarks and VVUQ test suites in general be well-documented and widely promulgated. If they are not well understoodd and easy to use, they are of limited value. 2.2 Approach VVUQ is a process that must continue as long as computational models are being developed and applied. While some steps are straightforward, it is not simple or quick for models as complex as those used by CHL and ERDC. As pointed out in [Oberkampf and Trucano 2008, DMSCO, 2011], the limiting factors for VVUQ are typically time and or money rather than completeness of the analyses. As such, it is important to determine where our computational models are currently in the VVUQ process and what resources they have for VVUQ. To begin this assessment, the VVUQ team developed a survey of VVUQ activity (Table A1). For the first round of assessment, the survey was disseminated to key models in the Flood Risk Management Research Program. Table 2.1: Table A1. Survey questions regarding VVUQ. 1. Name, Lab, contact info. 2. Name of the model. 3. Brief description of the model functioning. 4. What verification test problems are available? How are they documented? 5. What validation test problems are available? How are they documented? 6. Are any of the existing V&amp;V test cases included in a form of regular testing frameworks? If so, what existing framework/tool/software is being used? 7. Does the code include a suite of tests for identifying software bugs/errors (i.e., regressions)? If you are using a third party tool for this, please identify that tool. 8. What level of sensitivity analysis and/or uncertainty quantification is integrated in the software/technology suite? 9. Are VV and UQ an integral part of current model execution? For example, do users receive some quantification of uncertainty as part of the model output? Are explicit guidelines and procedures for sensitivity analysis and uncertainty quantification provided for users of the software? Are these provided in technical reports and/or code documentation? 10. Are the VVUQ test suites made available to users? If so, how? 11. What are the estimated costs (dollars and person hours) for developing the VVUQ suite? 12. How much are you currently expending in terms of funds and manpower to support VVUQ? Is VVUQ explicitly supported in any of your funding? 13. What is the estimated cost of continuing your current VVUQ efforts? How much capacity exists in terms of expertise and availability of personnel for supporting additional VVUQ efforts? Are you currently doing VVUQ via outside contracts, or PETTT projects, community based, or something else? 14. What are the biggest gaps in the VVUQ suite? What do you see as the highest priority needs for your particular code and the lab in general? 15. What mechanism do you recommend for disseminating test databases and documentation? 16. Do you have suggestions for the stewardship and governance of the lab’s VVUQ suites? 17. What other suggestions do you have for the VVUQ effort? 2.3 Results Of the sixteen surveys requested, responses were received from ten model teams: ADCIRC, AutoRoute and the Streamflow Prediction Tool (SPT), AdH, FUNWAVE, GSMB, GSSHA, HEC-RAS, HEC-FDA, HEC-WAT, and WaveWatch3. The remaining teams will be asked to submit their responses at a later date. Below we summarize the responses collectively to identify common trends and notable exceptions. Full responses are included in the appendix. The level of VVUQ activities varied widely across the models. In some cases, verification and validation are limited to a handful of benchmarks that are manually administered with minimal documentation. Most models have verification and validation through published reports and user documentation. Fewer include VV benchmarks in an automated testing framework, and still fewer release benchmark test input and data with their software. Several models include some form of automated unit and regression testing. However, access or use of a full development pipeline with automated testing was identified as a key gap by several models. The uncertainty quantification aspect of VVUQ appears to lag significantly behind verification and validation. Some models like HEC-WAT and HEC-FDA have uncertainty quantification as a part of their primary design. Others, like HEC-RAS and GSSHA include sensitivity analysis and Monte-Carlo capabilities for addressing parameter uncertainty, but fully integrated UQ is limited as a rule. Guidance for addressing uncertainty is generally not provided formally or available through user manuals and published technical reports. There was consensus that the best way to disseminate VVUQ benchmarks and information was through web-sites or software portals together with the released code. Right now, most models do not provide benchmark input/output files directly to users via the web. Collectively among the responding model groups, well over a million dollars has already been spent developing the existing VVUQ capabilities. The estimated cost to maintain existing VVUQ efforts or increase them to an acceptable level ranged from roughly 30K to 100K per year, with most models estimating around 50K per year. This estimate seems high and will likely drop once the initial test cases have been created and automatic regression testing software and other automated processes put into place. Likely, the annual cost per model will be on the order of $10k. There was also general agreement that VVUQ benchmarks are needed and could be a useful way of collaborating among modeling groups. Model-specific input files should be provided, but it was agreed that benchmark problems would be best distributed through model agnostic descriptions with model independent data. The gaps identified by the modeling teams were: availability of high-quality data for validation, available personnel with VVUQ expertise, and clearly defined policy and tools for automated testing along with tools for user interface testing and engaging users in testing procedures. In summary, the state of VVUQ in the surveyed models is a mixed bag. The need for VVUQ is recognized universally, although some groups have made more progress than others. There is agreement that cross-model VVUQ efforts are needed, both to provide generality and broader coverage, while also encouraging greater collaboration. 2.4 Recommendations Many recommendations for best practices in VVUQ can be found in the literature. The work of Oberkampf and collaborators is notable [Oberkampf and Trucano, 2008, Oberkampf and Roy 2010], but there is a rich literature spanning several fields. In a military context, guidelines and templates for VV and accreditation are available from the DMSCO, while guidance on software development practices can also be found in recent efforts to bring improved software engineering practice to scientific computing and groups advocating practices for reproducible science [Nature, 2018]. Based on the survey responses and discussions among the NTMS’s VVUQ focus group, we make the following recommendations for next steps for the VVUQ component of the NTMS. First, a standardized set of verification and validation benchmarks are needed. These tests should be grouped by physical process and dynamical regime rather than by model. Designing the benchmarks should be done as a collaboration among model developers, users, and experimentalists and should follow recognized best practices [e.g., Oberkampf and Trocano, 2008]. In particular, the benchmarks must (1) be well-documented in a model agnostic way, (2) have clearly defined metrics for code evaluation, and (3) widely shared via a web-based platform. We also recommend that individual models provide input decks and expected output results in connection with the benchmarks as well as a document describing any particular applicability issues for the given model. Current efforts within the NTMS in developing unified data formats should be leveraged in determining the best format for distributing benchmark tests. VVUQ practice has been driven to this point by individual model development groups. While these teams should obviously still be central in the VVUQ process, shared resources and direction are needed from the lab and technical programs. This includes manpower and funding resources. Resources need to be allocated to the model development teams. However, for VVUQ to be a uniform practice across the lab a small group with identified responsibility for VVUQ practice is likely needed. This group would also require adequate resources and expertise. Furthermore, this group should in general be made up of team members separate from the model develop teams. This separate grouping of testers and developers helps to ensure the integrity of the overall process and aids in the accreditation step to be discussed later. Concrete steps could include identifying a VVUQ focused work unit in the technical programs, requiring VVUQ to be addressed in all modeling related PMPs, and targeting hires in the field of VVUQ. Given funding and manpower constraints, it makes sense to pick an initial set of processes and models to evaluate. A logical choice in our opinion would be to consider wave and circulation models. First, these models are central to much of CHL’s civil works and military missions. Second, the Coastal Model Testbed effort already provides an excellent opportunity to engage modelers, experimentalists, and scientists with experts in field observations. Finally, we have spent relatively little time discussing accreditation. However, this is an essential step for models to be considered fully mature and is now required for certain classes of applications. The DMSCO provides guidance and examples of accreditation (or certification) procedures in the DoD Modeling and Simulation community. Some of these procedures may not be a perfect fit for the classes of models developed and supported by the lab, and there exist other certification procedures more appropriate for civil works applications which some of the surveyed models have already undergone (and continue to undergo). Accreditation of users for models is also important, maybe more so than an actual model accreditation. Expert tools in the hands of untrained users can be very dangerous. It is recommend that CHL develop a series of user certification classes for its models. These classes would be offered to USACE districts, military users as well as made available to academic institutions and external government and private sector contractors. It is recommended that software, infrastructure and suitable training be put in place for automatic regression testing and feature/bug tracking. Some modeling groups are already using some forms of automated regression testing, such as the AdH development team. For issue tracking and feature tasks, applications like Jira or Gitlab’s internal issue tracker can be used. Automated regression testing software such as CTest, from Kitware, or others readily available through the Defense Intelligence Information Enterprise (DI2E). This is an area with the Information Technology Lab (ITL) or the DoD High Performance Computing Modernization Program (HPCMP) could help to make recommendations for software to use, help to set up use cases and possibly provide training. Finally, the next phase is to develop a comprehensive plan for implementing these recommendations. This plan will need to have cost and times associated with each recommendation along with an order of priority for implementation for specific operational models. 2.5 References Asch, M., M. Bocquet, and M. Nodet, Data Assimilation: Methods, Algorithms, and Applications, SIAM, 2016. Challenges in Irreproducible Research, Special Issue, Nature 2018. Defense Modeling and Simulation Coordination Office (DMSCO), website https://vva.msco.mil, published 2011. Katz, A. and V. Sankaran, Mesh Quality Effects on the Accuracy of CFD solutions on Unstructured Meshes, Journal of Computational Physics, Volume 230, Issue 20 pages 7670-7686, 2011. Oberkampf, W.L. and C.J. Roy, Verification and Validation in Scientific Computings, Cambridge Scientific, 2010. Oberkampf, W.L. and T.G. Trucano, Verification and Validation in Computational Fluid Dynamics, Progress in Aerospace Sciences, Volume 38, pages 209-272, 2002. Oberkampf, W.L. and T.G. Trucano, Verification and Validation Benchmarks, Nuclear Engineering and Design, Volume 238, Issue 3, pages 716-743, 2008. Piccolo, S.R. and M.B. Frampton, Tools and techniques for computational reproducibility, Gigascience, Volume 5, doi 10.1186/s13742-016-0135-4 2016. Roache, P.J., Quantification of uncertainty in computational fluid dynamics, Annual Review of Fluid Mechanics, pages. 126–160 "],["component-based-software-design-teams-whitepaper-for-the-numerical-technology-modernization-program.html", "Chapter 3 Component-Based Software Design Team’s Whitepaper for the Numerical Technology Modernization Program", " Chapter 3 Component-Based Software Design Team’s Whitepaper for the Numerical Technology Modernization Program Version: 1.0.0 Date: 2019.03.18 Written by: Corey Trahan, Lucas Pettey, and Chris Massey (Chris.Massey@usace.army.mil) The Component-Based Software Design Team was created as part of the overall Numerical Technology Modernization Program (NTMS). Add other text later. "],["appendix-tbd.html", "Chapter 4 Appendix TBD", " Chapter 4 Appendix TBD Version: 1.0.0 Date: 2019.03.18 Written by: authors (email) Add other text later. "],["appendix-tbd-1.html", "Chapter 5 Appendix TBD", " Chapter 5 Appendix TBD Version: 1.0.0 Date: 2019.03.18 Written by: authors (email) Add other text later. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
